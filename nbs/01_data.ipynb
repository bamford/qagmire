{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0493e6",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Functionality for efficiently accessing data in WEAVE fits files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36d3a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd82e7d2-e93b-429f-afb2-3553ec8a026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19db9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from functools import partial, wraps\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "from tqdm import tqdm\n",
    "\n",
    "from qagmire.utilities import mjd_to_night"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bfd6dc-e4e5-4f19-a01e-b15e66053908",
   "metadata": {},
   "source": [
    "The `data` module provides functions for locating and reading WEAVE data as `xarray` `Dataset`s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df6f925",
   "metadata": {},
   "source": [
    "## Accessing FITS tables as xarray Datasets via cached netCDF files\n",
    "\n",
    "The approach of `qagmire` is to access WEAVE data stored on disk in FITS files on disk. To analyse this large, multi-dimensional dataset we utilize [`xarray`](https://docs.xarray.dev), making use of its ability to use `dask` to perform computations in parallel in a memory efficient and scaleable manner. To make this work we need to write the data to disk in a suitable format: netCDF files, then load those files as an `xarray.Dataset`. This functionality is implemented in `FITStoDataset`, which is used to wrap simpler functions which focus on reading data from a single FITS file into a convenient `Dataset`. Reading the original FITS files, and caching them to NetCDF, is (by default) parallelised using `multiprocessing`. In general, the aim is for the resulting `Dataset`s to preserve the structure in the FITS files. However, there are some cases where it is sensible to rearrange the data into a more convenient and/or efficient format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93627763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def _read_single(\n",
    "    read_function: Callable[[str], xr.Dataset],  # function to read a FITS file\n",
    "    fn: str,  # filename of FITS file to read\n",
    "):\n",
    "    \"\"\"Read a FITS file to an xarray Dataset using the given read function.\"\"\"\n",
    "    ds = read_function(fn)\n",
    "    if ds:\n",
    "        fn_base = os.path.splitext(os.path.basename(fn))[0]\n",
    "        ds = ds.expand_dims({\"filename\": [fn_base]})\n",
    "        try:\n",
    "            run = fits.getval(fn, \"RUN\")\n",
    "            ds = ds.assign_coords(RUN=(\"filename\", [run]))\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            camera = fits.getval(fn, \"CAMERA\")\n",
    "            camera = camera.replace(\"WEAVE\", \"\")\n",
    "            ds = ds.assign_coords(CAMERA=(\"filename\", [camera]))\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            mjd = np.round(fits.getval(fn, \"MJD-OBS\"), 4)\n",
    "            ds = ds.assign_coords(MJD=(\"filename\", [mjd]))\n",
    "            night = mjd_to_night(mjd)\n",
    "            ds = ds.assign_coords(NIGHT=(\"filename\", [night]))\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            obid = fits.getval(fn, \"OBID\")\n",
    "            ds = ds.assign_coords(OBID=(\"filename\", [obid]))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    else:\n",
    "        table = read_function.__name__.replace(\"read_\", \"\")\n",
    "        print(f\"Warning: cannot read {table} for file {fn}.\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def _single_via_netcdf(\n",
    "    read_function: Callable[[str], xr.Dataset],  # function to read a FITS file\n",
    "    fn: str,  # filename of FITS file to read\n",
    "    netcdf_store,\n",
    "    update_cache,\n",
    "):\n",
    "    \"\"\"Transform a FITS file to netCDF using the given read function.\n",
    "\n",
    "    Returns the path of a netCDF file stored in `nedcdf_store`, containing the `Dataset` resulting\n",
    "    from calling `read_function` with the supplied FITS filename `fn`. If the netCDF file already\n",
    "    exists, the filename is immediately returned.\n",
    "    \"\"\"\n",
    "    fn_netcdf = os.path.join(netcdf_store, *os.path.normpath(fn).split(os.sep)[-3:])\n",
    "    table = read_function.__name__.replace(\"read_\", \"\")\n",
    "    fn_netcdf = os.path.splitext(fn_netcdf)[0]\n",
    "    fn_netcdf = f\"{fn_netcdf}_{table}.nc\"\n",
    "    if not os.path.exists(fn_netcdf) or update_cache:\n",
    "        ds = _read_single(read_function, fn)\n",
    "        if ds:\n",
    "            os.makedirs(os.path.dirname(fn_netcdf), exist_ok=True)\n",
    "            ds.to_netcdf(fn_netcdf, format=\"NETCDF4\", engine=\"netcdf4\")\n",
    "            ds.close()\n",
    "        else:\n",
    "            fn_netcdf = None\n",
    "    return fn_netcdf\n",
    "\n",
    "\n",
    "class FITStoDataset:\n",
    "    \"\"\"Access multiple FITS tables as an xarray Dataset, optionally via cached netCDF files.\n",
    "\n",
    "    For each FITS table or image we wish to read, we will write a `read_*(fn)` function\n",
    "    which reads the table from the single provided filename `fn` and returns a `Dataset`.\n",
    "    Wrapping such a function with an instance of this class adapts the function to take\n",
    "    a list of FITS filenames and return a `Dataset`. If `cache=True`, the Dataset is\n",
    "    lazily loaded data from a cache of netCDF files. The cache is stored in the\n",
    "    `netcdf_store` folder defined when the instance is initialised.\n",
    "\n",
    "    If `cache=True`, when the wrapped function is initially run, it repeatedly calls\n",
    "    `single_via_netcdf` to apply the original `read_*` function to each FITS filename and\n",
    "    save each resulting `Dataset` as a netCDF file, then opens them together and returns a\n",
    "    combined, distributed `Dataset`. If a previously converted file is found in the\n",
    "    `netcdf_store`, then the original `read_*` function is skipped and the netCDF file loaded\n",
    "    directly. This caching can vastly increase the speed of subsequent calls.\n",
    "    If `n_processes > 1`, which it is by default, then this reading and caching is performed\n",
    "    in parallel using `n_processes` processes.\n",
    "\n",
    "    Although instances of this class can be used as a decorator, doing so with\n",
    "    `n_processes > 1` will lead to an exception due to pickling issues with multiprocessing.\n",
    "    Instead they should be used to wrap functions, without replacing the original function name.\n",
    "    For example,\n",
    "    ```\n",
    "    to_dataset = FITStoDataset()\n",
    "\n",
    "    def _class_spec_reader(fn):\n",
    "        ...\n",
    "\n",
    "    read_class_spec = to_dataset(_class_spec_reader)\n",
    "    ```\n",
    "\n",
    "    If a source FITS file is changed, the corresponding files in `netcdf_store` can simply\n",
    "    be deleted and they will be recreated on the next call of the decorated `read_*`\n",
    "    function.\n",
    "\n",
    "    If `cache=False`, then the data is always read from the specified FITS files, combined\n",
    "    and returned as an in-memory Dataset. This may be faster when dealing with lots of small\n",
    "    files.\n",
    "\n",
    "    If `update_cache=False`, then existing cache files are not read, but are recreated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cache=True,  # cache the dataset to netCDF files\n",
    "        netcdf_store: str | None = None,  # folder in which to store the netCDF files\n",
    "        progress=True,  # display a progress bar\n",
    "        update_cache=False,  # read FITS files and recreate netCDF files, no effect if cache=False\n",
    "        n_processes=8,  # how many subprocesses to use\n",
    "    ):\n",
    "        \"\"\"Create a decorator that can extend a `read_*` function to multiple files.\n",
    "\n",
    "        If no `netcdf_store` is provided it first checks for a `NETCDF_STORE` environment\n",
    "        variable and falls back to a folder called `netcdf_store` in the user's home folder.\n",
    "        \"\"\"\n",
    "        self.cache = cache\n",
    "        self.update_cache = update_cache\n",
    "        self.n_processes = n_processes\n",
    "\n",
    "        if netcdf_store is not None:\n",
    "            self.netcdf_store = netcdf_store\n",
    "        else:\n",
    "            default = \"/beegfs/weavelofar/netcdf_store\"\n",
    "            self.netcdf_store = os.environ.get(\"NETCDF_STORE\", default)\n",
    "\n",
    "        if progress:\n",
    "            if cache:\n",
    "                desc = \"Locating and converting where necessary\"\n",
    "            else:\n",
    "                desc = \"Reading files\"\n",
    "            self.progress = partial(tqdm, desc=desc, file=sys.stdout)\n",
    "        else:\n",
    "            self.progress = lambda x: x\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        read_function: Callable[[str], xr.Dataset],  # function to read a FITS file\n",
    "    ):\n",
    "        \"\"\"Extend the functionality of `read_function` to multiple files.\n",
    "\n",
    "        The wrapped `read_function` is adapted to take a list of FITS filenames and\n",
    "        return a `Dataset`, which lazily loads data from a cache of netCDF files if\n",
    "        `self.cache=True` (the default).\n",
    "        \"\"\"\n",
    "\n",
    "        @wraps(read_function)\n",
    "        def wrapper(fns):\n",
    "            if self.cache:\n",
    "                read = partial(\n",
    "                    _single_via_netcdf,\n",
    "                    read_function,\n",
    "                    netcdf_store=self.netcdf_store,\n",
    "                    update_cache=self.update_cache,\n",
    "                )\n",
    "            else:\n",
    "                read = partial(_read_single, read_function)\n",
    "\n",
    "            if self.n_processes > 1:\n",
    "                results = []\n",
    "                try:\n",
    "                    with Pool(self.n_processes) as p:\n",
    "                        with self.progress(total=len(fns)) as pbar:\n",
    "                            for f in p.imap_unordered(read, fns):\n",
    "                                results.append(f)\n",
    "                                pbar.update()\n",
    "                except PermissionError:\n",
    "                    raise PermissionError(\n",
    "                        \"Cannot access the NetCDF file. Ensure any previously \"\n",
    "                        \"created Datasets are closed, e.g. ds.close()\"\n",
    "                    )\n",
    "            else:\n",
    "                results = [read(fn) for fn in self.progress(fns)]\n",
    "\n",
    "            results = [f for f in results if f is not None]\n",
    "\n",
    "            if self.cache:\n",
    "                print(\"Reading netCDF files... \", end=\"\")\n",
    "                start = time.perf_counter()\n",
    "                data = xr.open_mfdataset(\n",
    "                    results,\n",
    "                    combine=\"nested\",\n",
    "                    coords=\"minimal\",\n",
    "                    concat_dim=\"filename\",\n",
    "                    engine=\"netcdf4\",\n",
    "                )\n",
    "            else:\n",
    "                print(\"Creating Dataset... \", end=\"\")\n",
    "                start = time.perf_counter()\n",
    "                data = xr.concat(results, dim=\"filename\", coords=\"minimal\")\n",
    "            dt = time.perf_counter() - start\n",
    "            print(f\"took {dt:.2f} s. Size is {data.nbytes * 2**-20:.3f} Mb\")\n",
    "            return data\n",
    "\n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd8c3c-bd48-4da6-bf4c-f0441ceb1dc0",
   "metadata": {},
   "source": [
    "### Create the `to_dataset` and `to_dataset_without_cache` wrapping functions\n",
    "\n",
    "These are used to wrap all the `read_*` functions defined below. The function `to_dataset` is an instance of `FITStoDataset` with all the default behaviour described above.\n",
    "\n",
    "For datasets consisting of small amounts of data per FITS file, it appears to be more efficient to simply read from the FITS files every time. In such cases, we use `to_dataset_without_cache`, an instance of `FITStoDataset` with caching disabled.\n",
    "\n",
    "Note that both of must be used to wrap functions, with the result being assigned a different name to the original. They cannot be used as decorators, unless `n_processes=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b38dd71c-4eca-4d0a-9f02-045c5b2b8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "\n",
    "to_dataset = FITStoDataset()\n",
    "to_dataset_without_cache = FITStoDataset(cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5fcee4-f372-48ab-90e0-c73d23712c05",
   "metadata": {},
   "source": [
    "Existing cache files will be used when using the `qagmire.data` module. However, if running this notebook, the following line means we always update the cache, such that the timings reflect the original reading and conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a0b3794-942a-45a5-949a-9ddd36b106ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dataset = FITStoDataset(update_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a56a7bc",
   "metadata": {},
   "source": [
    "## Locating WEAVE FITS files\n",
    "\n",
    "Here we define some functions to get lists of WEAVE FITS filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36e37570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "data_path = \"/beegfs/weavelofar\"\n",
    "\n",
    "def _is_lowres(fn):\n",
    "    \"\"\"Check the header of FITS file `fn` to determine if it is low-resolution.\"\"\"\n",
    "    try:\n",
    "        lowres = \"LR\" in fits.getval(fn, \"RES-OBS\")\n",
    "    except KeyError:\n",
    "        lowres = \"LOWRES\" in fits.getval(fn, \"MODE\")\n",
    "    return lowres\n",
    "\n",
    "\n",
    "def get_weave_files(\n",
    "    level=\"*\",  # pattern to match to the file level, e.g. raw, L1, L2\n",
    "    filetype=\"*\",  # pattern to match to the file type, e.g. single, stack\n",
    "    date=\"*\",  # pattern to match to the date in format yyyymmdd\n",
    "    runid=\"*\",  # pattern to match to the runid\n",
    "    lowres=True,  # select low-res files, or high-res if False\n",
    "    folder=\"weaveio\",  # folder within the `data_path`\n",
    "):\n",
    "    \"\"\"Get a list of matching WEAVE files.\"\"\"\n",
    "    if level != \"raw\":\n",
    "        filetype += \"_\"\n",
    "    level = \"\".join(f\"[{c.upper()+c.lower()}]\" for c in level)\n",
    "    pattern = f\"**/{level}/{date}*/{filetype}*{runid}*.fit*\"\n",
    "    pattern = os.path.join(data_path, folder, pattern)\n",
    "    files = glob(pattern, recursive=True)\n",
    "    files.sort()\n",
    "    if lowres:\n",
    "        files = [fn for fn in files if _is_lowres(fn)]\n",
    "    else:\n",
    "        files = [fn for fn in files if not _is_lowres(fn)]\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_lr_raw_files(\n",
    "    date=\"*\",  # pattern to match to the date in format yyyymmdd\n",
    "    runid=\"*\",  # pattern to match to the runid\n",
    "    folder=\"weaveio\",  # folder within the `data_path`\n",
    "):\n",
    "    return get_weave_files(level=\"raw\", date=date, runid=runid, lowres=True, folder=folder)\n",
    "\n",
    "\n",
    "def get_lr_l1_single_files(\n",
    "    date=\"*\",  # pattern to match to the date in format yyyymmdd\n",
    "    runid=\"*\",  # pattern to match to the runid\n",
    "    folder=\"weaveio\",  # folder within the `data_path`\n",
    "):\n",
    "    return get_weave_files(\n",
    "        level=\"L1\", filetype=\"single\", date=date, runid=runid, lowres=True, folder=folder\n",
    "    )\n",
    "\n",
    "\n",
    "def get_lr_l1_stack_files(\n",
    "    date=\"*\",  # pattern to match to the date in format yyyymmdd\n",
    "    runid=\"*\",  # pattern to match to the runid\n",
    "    folder=\"weaveio\",  # folder within the `data_path`\n",
    "):\n",
    "    return get_weave_files(\n",
    "        level=\"L1\", filetype=\"stack\", date=date, runid=runid, lowres=True, folder=folder\n",
    "    )\n",
    "\n",
    "\n",
    "def get_lr_l2_stack_files(\n",
    "    date=\"*\",  # pattern to match to the date in format yyyymmdd\n",
    "    runid=\"*\",  # pattern to match to the runid\n",
    "    folder=\"weaveio\",  # folder within the `data_path`\n",
    "):\n",
    "    return get_weave_files(\n",
    "        level=\"L2\", filetype=\"stack\", date=date, runid=runid, lowres=True, folder=folder\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66785a65-b242-4a17-a8f2-97592aa0dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def _read_fits_columns(\n",
    "    fn: str,  # the filename of the FITS file to read\n",
    "    ext: str,  # the name of the extension containing the table to read\n",
    "    limit_precision=False,  # convert all float64 columns to float32\n",
    "    index: str | None = None,  # remove rows where this column is masked\n",
    "):\n",
    "    \"\"\"Read a FITS table to a dict of arrays and convert endianness.\"\"\"\n",
    "    cols = dict(Table.read(fn, ext, unit_parse_strict=\"silent\"))\n",
    "    cols = {c: cols[c].newbyteorder().byteswap() for c in cols}\n",
    "    if limit_precision:\n",
    "        for c in list(cols):\n",
    "            if cols[c].dtype.type is np.float64:\n",
    "                if np.can_cast(np.max(np.abs(cols[c])), np.float32):\n",
    "                    cols[c] = cols[c].astype(np.float32)\n",
    "    if index is not None:\n",
    "        ok = ~cols[index].mask\n",
    "        cols = {c: cols[c][ok] for c in cols}\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c1a08-2186-4932-938e-500deccbcf4b",
   "metadata": {},
   "source": [
    "## Raw files\n",
    "\n",
    "These contain the raw observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34b884-7201-486a-8dcd-b2e5329bace9",
   "metadata": {},
   "source": [
    "We will read some simulated files to show examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01681c4f-2fbe-4490-a080-aaff543eae0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/beegfs/weavelofar/weaveio/**/[Rr][Aa][Ww]/2017*/****.fit*\n",
      "120 low-res raw files\n"
     ]
    }
   ],
   "source": [
    "lr_raw_files = get_lr_raw_files(date=\"2017*\")\n",
    "print(len(lr_raw_files), \"low-res raw files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c89000-1e8c-4a18-ba7d-37bced2c3451",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_hdus = fits.open(lr_raw_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558df03-6ec6-4ec0-bae8-09070797b688",
   "metadata": {},
   "source": [
    "WEAVE raw files contain six extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603c814-234c-46d8-a6f6-58d95f3351a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRIMARY', 'RED1_DATA', 'RED2_DATA', 'FIBTABLE', 'GUIDINFO', 'METINFO']\n"
     ]
    }
   ],
   "source": [
    "print([hdu.name for hdu in raw_hdus])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a865f33-a13b-4df3-bba3-7cd148e9f21a",
   "metadata": {},
   "source": [
    "### PRIMARY\n",
    "\n",
    "The PRIMARY extension contains only a header with lots of information about the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a904b4dc-2709-4fb2-81be-176c1b225c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SIMPLE  =                    T / conforms to FITS standard                      \n",
       "BITPIX  =                    8 / array data type                                \n",
       "NAXIS   =                    0 / number of array dimensions                     \n",
       "EXTEND  =                    T                                                  \n",
       "COMMENT -------- Start of the CAMERA Packet -------                             \n",
       "RUN     =              1003313                                                  \n",
       "IRAFNAME= 'r1003313'           / redir r2840373.fit > r1003313                  \n",
       "DETECTOR= 'WVRED   '           / Selected by inference                          \n",
       "CCDSPEED= 'SLOW    '                                                            \n",
       "CCDXBIN =                    1                                                  \n",
       "CCDYBIN =                    1                                                  \n",
       "CCDSUM  = '1 1     '                                                            \n",
       "CCDTEMP =    132.0555378866794 / Randomly generated                             \n",
       "CCDTEMP1=    124.5350077054515 / Randomly generated                             \n",
       "CCDTEMP2=    127.1948417375027 / Randomly generated                             "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_hdus[\"PRIMARY\"].header[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07870f57-75d4-4e78-a3d4-0faf17dae7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def _primary_header_reader(fn):\n",
    "    \"\"\"Read the primary header as a Dataset, stripping comments.\"\"\"\n",
    "    hdr = fits.getheader(fn, \"PRIMARY\")\n",
    "    for key in hdr:\n",
    "        if key == \"\" or \"COMM\" in key:\n",
    "            del hdr[key]\n",
    "    return xr.Dataset(hdr)\n",
    "\n",
    "\n",
    "read_primary_header = to_dataset_without_cache(_primary_header_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea1ba3-24a1-45fd-9831-5f26fdd01216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L310){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_primary_header\n",
       "\n",
       ">      read_primary_header (fn)\n",
       "\n",
       "Read the primary header as a Dataset, stripping comments."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L310){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_primary_header\n",
       "\n",
       ">      read_primary_header (fn)\n",
       "\n",
       "Read the primary header as a Dataset, stripping comments."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(read_primary_header, name=\"read_primary_header\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328b401-4e20-4362-a5bc-a308ee723a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████| 120/120 [00:01<00:00, 90.27it/s]\n",
      "Creating Dataset... took 1.89 s. Size is 0.604 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  22%|███████▏                         | 26/120 [00:00<00:01, 92.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  30%|█████████▉                       | 36/120 [00:00<00:00, 90.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  38%|████████████▋                    | 46/120 [00:00<00:01, 63.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  45%|██████████████▊                  | 54/120 [00:00<00:01, 64.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  53%|█████████████████▌               | 64/120 [00:00<00:00, 69.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  63%|████████████████████▉            | 76/120 [00:01<00:00, 75.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  72%|███████████████████████▉         | 87/120 [00:01<00:00, 77.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  82%|██████████████████████████▉      | 98/120 [00:01<00:00, 75.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  88%|████████████████████████████▎   | 106/120 [00:01<00:00, 63.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  98%|███████████████████████████████▏| 117/120 [00:01<00:00, 72.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files: 100%|████████████████████████████████| 120/120 [00:04<00:00, 25.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Dataset... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 1.67 s. Size is 0.604 Mb\n"
     ]
    }
   ],
   "source": [
    "raw_primary_header = read_primary_header(lr_raw_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794c588-c1ca-4132-8a8c-0c0840106c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 633kB\n",
      "Dimensions:   (filename: 120)\n",
      "Coordinates:\n",
      "  * filename  (filename) object 960B 'r1003313' 'r1003317' ... 'r1004149'\n",
      "    RUN       (filename) int64 960B 1003313 1003317 1003318 ... 1004151 1004149\n",
      "    CAMERA    (filename) <U4 2kB 'RED' 'RED' 'BLUE' ... 'BLUE' 'RED' 'RED'\n",
      "    OBID      (filename) int64 960B 3802 3802 3802 3802 ... 3936 3936 3936 3936\n",
      "    MJD       (filename) float64 960B 5.781e+04 5.781e+04 ... 5.803e+04\n",
      "    NIGHT     (filename) <U8 4kB '20170224' '20170224' ... '20170930' '20170930'\n",
      "Data variables: (12/410)\n",
      "    SIMPLE    (filename) bool 120B True True True True ... True True True True\n",
      "    BITPIX    (filename) int64 960B 8 8 8 8 8 8 8 8 8 8 ... 8 8 8 8 8 8 8 8 8 8\n",
      "    NAXIS     (filename) int64 960B 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0\n",
      "    EXTEND    (filename) bool 120B True True True True ... True True True True\n",
      "    IRAFNAME  (filename) <U8 4kB 'r1003313' 'r1003317' ... 'r1004151' 'r1004149'\n",
      "    DETECTOR  (filename) <U6 3kB 'WVRED' 'WVRED' 'WVBLUE' ... 'WVRED' 'WVRED'\n",
      "    ...        ...\n",
      "    CCNAME17  (filename) <U1 480B '' '' '' '' '' '' '' ... '' '' '' '' '' '' ''\n",
      "    CCNAME18  (filename) <U1 480B '' '' '' '' '' '' '' ... '' '' '' '' '' '' ''\n",
      "    CCNAME19  (filename) <U1 480B '' '' '' '' '' '' '' ... '' '' '' '' '' '' ''\n",
      "    CCNAME20  (filename) <U1 480B '' '' '' '' '' '' '' ... '' '' '' '' '' '' ''\n",
      "    CHECKSUM  (filename) <U16 8kB '6Zki8Wkh6Wkh6Wkh' ... '9LH8HK969KG6EK96'\n",
      "    DATASUM   (filename) <U10 5kB '         0' '         0' ... '         0'\n"
     ]
    }
   ],
   "source": [
    "print(raw_primary_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a86ebc-0b89-4530-a53c-9cb4c0f89b7a",
   "metadata": {},
   "source": [
    "### DATA\n",
    "\n",
    "The RED1_DATA, RED2_DATA and BLUE1_DATA, BLUE2_DATA extensions contain raw imaging of the spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327898af-4a1e-4f45-b97c-3dfdc0fc04a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def _raw_data_reader(fn):\n",
    "    \"\"\"Read the *_DATA from a WEAVE RAW FITS file as a Dataset.\"\"\"\n",
    "    hdus = fits.open(fn)\n",
    "    for h in hdus:\n",
    "        if h.name.endswith(\"1_DATA\"):\n",
    "            counts1 = xr.DataArray(h.data)\n",
    "        elif h.name.endswith(\"2_DATA\"):\n",
    "            counts2 = xr.DataArray(h.data)\n",
    "    return xr.Dataset({\"counts1\": counts1, \"counts2\": counts2})\n",
    "\n",
    "\n",
    "read_raw_data = to_dataset(_raw_data_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666810d-d14a-411d-ad9a-1b651521646a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L322){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_raw_data\n",
       "\n",
       ">      read_raw_data (fn)\n",
       "\n",
       "Read the *_DATA from a WEAVE RAW FITS file as a Dataset."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L322){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_raw_data\n",
       "\n",
       ">      read_raw_data (fn)\n",
       "\n",
       "Read the *_DATA from a WEAVE RAW FITS file as a Dataset."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(read_raw_data, name=\"read_raw_data\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13869ece-0c9f-46ee-acb5-bca7f75c1989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locating and converting where necessary: 100%|██████████| 120/120 [00:26<00:00,  4.58it/s]\n",
      "Reading netCDF files... took 2.92 s. Size is 17888.977 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:   8%|▌      | 10/120 [00:03<00:39,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  12%|▉      | 15/120 [00:03<00:20,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  15%|█      | 18/120 [00:05<00:29,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  18%|█▏     | 21/120 [00:05<00:21,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  19%|█▎     | 23/120 [00:05<00:18,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  21%|█▍     | 25/120 [00:07<00:28,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  22%|█▌     | 27/120 [00:07<00:23,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  25%|█▊     | 30/120 [00:08<00:27,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  26%|█▊     | 31/120 [00:08<00:25,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  28%|█▉     | 34/120 [00:09<00:19,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  31%|██▏    | 37/120 [00:10<00:22,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  32%|██▏    | 38/120 [00:10<00:20,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  33%|██▎    | 40/120 [00:10<00:17,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  35%|██▍    | 42/120 [00:10<00:13,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  36%|██▌    | 43/120 [00:10<00:13,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  37%|██▌    | 44/120 [00:11<00:21,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  38%|██▋    | 45/120 [00:11<00:19,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  38%|██▋    | 46/120 [00:11<00:18,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  39%|██▋    | 47/120 [00:12<00:17,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  42%|██▉    | 50/120 [00:12<00:10,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  42%|██▉    | 51/120 [00:12<00:16,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  43%|███    | 52/120 [00:13<00:17,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  45%|███▏   | 54/120 [00:13<00:13,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  46%|███▏   | 55/120 [00:13<00:13,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  48%|███▍   | 58/120 [00:13<00:08,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  49%|███▍   | 59/120 [00:14<00:16,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  51%|███▌   | 61/120 [00:15<00:14,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  52%|███▌   | 62/120 [00:15<00:13,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  54%|███▊   | 65/120 [00:15<00:08,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  55%|███▊   | 66/120 [00:16<00:13,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  56%|███▉   | 67/120 [00:16<00:12,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  57%|███▉   | 68/120 [00:16<00:10,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  57%|████   | 69/120 [00:16<00:11,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  58%|████   | 70/120 [00:17<00:11,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  60%|████▏  | 72/120 [00:17<00:07,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  62%|████▎  | 74/120 [00:17<00:09,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  62%|████▍  | 75/120 [00:18<00:10,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  63%|████▍  | 76/120 [00:18<00:10,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  65%|████▌  | 78/120 [00:18<00:08,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  68%|████▋  | 81/120 [00:18<00:05,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  68%|████▊  | 82/120 [00:19<00:09,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  69%|████▊  | 83/120 [00:20<00:10,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  71%|████▉  | 85/120 [00:20<00:07,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  73%|█████▏ | 88/120 [00:20<00:05,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  74%|█████▏ | 89/120 [00:21<00:08,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  75%|█████▎ | 90/120 [00:21<00:08,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  77%|█████▎ | 92/120 [00:21<00:06,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  78%|█████▍ | 94/120 [00:22<00:05,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  79%|█████▌ | 95/120 [00:22<00:07,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  80%|█████▌ | 96/120 [00:23<00:07,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  81%|█████▋ | 97/120 [00:23<00:06,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  82%|█████▊ | 99/120 [00:23<00:03,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  83%|█████ | 100/120 [00:23<00:04,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  84%|█████ | 101/120 [00:24<00:06,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  85%|█████ | 102/120 [00:24<00:05,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  86%|█████▏| 103/120 [00:25<00:05,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  88%|█████▎| 105/120 [00:25<00:03,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  88%|█████▎| 106/120 [00:25<00:02,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  89%|█████▎| 107/120 [00:25<00:03,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  90%|█████▍| 108/120 [00:26<00:02,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  92%|█████▌| 110/120 [00:26<00:02,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  92%|█████▌| 111/120 [00:26<00:02,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  94%|█████▋| 113/120 [00:26<00:01,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  95%|█████▋| 114/120 [00:27<00:01,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  97%|█████▊| 116/120 [00:27<00:00,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  98%|█████▊| 117/120 [00:27<00:00,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  98%|█████▉| 118/120 [00:28<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  99%|█████▉| 119/120 [00:28<00:00,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary: 100%|██████| 120/120 [00:30<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary: 100%|██████| 120/120 [00:30<00:00,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading netCDF files... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 2.40 s. Size is 17888.977 Mb\n"
     ]
    }
   ],
   "source": [
    "raw_data = read_raw_data(lr_raw_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2822a5ba-a41b-4937-b6e9-bd1a7ed984b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 19GB\n",
      "Dimensions:   (filename: 120, dim_0: 6160, dim_1: 6344)\n",
      "Coordinates:\n",
      "  * filename  (filename) <U8 4kB 'r1003315' 'r1003314' ... 'r1004151' 'r1004152'\n",
      "    RUN       (filename) int64 960B dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "    CAMERA    (filename) <U4 2kB dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "    MJD       (filename) float64 960B dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "    NIGHT     (filename) <U8 4kB dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "    OBID      (filename) int64 960B dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "Dimensions without coordinates: dim_0, dim_1\n",
      "Data variables:\n",
      "    counts1   (filename, dim_0, dim_1) uint16 9GB dask.array<chunksize=(1, 6160, 6344), meta=np.ndarray>\n",
      "    counts2   (filename, dim_0, dim_1) uint16 9GB dask.array<chunksize=(1, 6160, 6344), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6b626-ea02-48db-8569-091dc61d7962",
   "metadata": {},
   "source": [
    "It is a good idea to close a `Dataset` when you are finished with it, as otherwise other processes may not be able to access the same underlying files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e277d-7364-4ccd-ad26-8d3f675bd586",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f11c0-ecbe-403a-a5cd-b59333c9bc92",
   "metadata": {},
   "source": [
    "### FIBTABLE\n",
    "\n",
    "The FIBTABLE extension contains information about the fibre allocations.\n",
    "\n",
    "Code is included to display example content of FITS tables, but commented out as it does not display well online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b56c058-4c3c-4cde-a3d3-94a17bb87c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table.read(raw_hdus[\"FIBTABLE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6db437-776f-4b07-b2da-3872d1cbe2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def _fibre_table_reader_indexed(fn, index_by_nspec=True):\n",
    "    cols = _read_fits_columns(fn, \"FIBTABLE\", index=\"FIBREID\")\n",
    "    cols = {c.upper(): cols[c] for c in cols}\n",
    "    if not cols:\n",
    "        return None\n",
    "    coords = dict(APS_ID=cols.pop(\"FIBREID\"))\n",
    "    for c in cols:\n",
    "        dims = [\"APS_ID\"]\n",
    "        cols[c] = xr.Variable(dims, cols[c], attrs={\"unit\": str(cols[c].unit)})\n",
    "    ds = xr.Dataset(cols, coords)\n",
    "    if index_by_nspec:\n",
    "        ds = ds.set_coords(\"NSPEC\").swap_dims(APS_ID=\"NSPEC\").reset_coords(\"APS_ID\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def _fibre_table_reader(fn):\n",
    "    \"\"\"Read the FIBTABLE from a WEAVE RAW or L1 FITS file as a Dataset.\n",
    "\n",
    "    All quantities are indexed by the `APS_ID` of the fibre.\n",
    "\n",
    "    All column names are mde uppercase for consistency.\n",
    "    \"\"\"\n",
    "    return _fibre_table_reader_indexed(fn, index_by_nspec=False)\n",
    "\n",
    "\n",
    "def _fibre_table_reader_nspec(fn):\n",
    "    \"\"\"Read the FIBTABLE from a WEAVE L1 FITS file as a Dataset.\n",
    "\n",
    "    All quantities are indexed by the `NSPEC` of the fibre.\n",
    "\n",
    "    All column names are mde uppercase for consistency.\n",
    "    \"\"\"\n",
    "    return _fibre_table_reader_indexed(fn, index_by_nspec=True)\n",
    "\n",
    "\n",
    "read_fibre_table = to_dataset_without_cache(_fibre_table_reader)\n",
    "read_fibre_table_nspec = to_dataset_without_cache(_fibre_table_reader_nspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c719e9f-1924-4786-8e79-ccda085f4a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L351){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_fibre_table\n",
       "\n",
       ">      read_fibre_table (fn)\n",
       "\n",
       "Read the FIBTABLE from a WEAVE RAW or L1 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "\n",
       "All column names are mde uppercase for consistency."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L351){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_fibre_table\n",
       "\n",
       ">      read_fibre_table (fn)\n",
       "\n",
       "Read the FIBTABLE from a WEAVE RAW or L1 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "\n",
       "All column names are mde uppercase for consistency."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(read_fibre_table, name=\"read_fibre_table\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76e044-0df7-493a-aad9-13507981c967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L361){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_fibre_table_nspec\n",
       "\n",
       ">      read_fibre_table_nspec (fn)\n",
       "\n",
       "Read the FIBTABLE from a WEAVE L1 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `NSPEC` of the fibre.\n",
       "\n",
       "All column names are mde uppercase for consistency."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L361){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_fibre_table_nspec\n",
       "\n",
       ">      read_fibre_table_nspec (fn)\n",
       "\n",
       "Read the FIBTABLE from a WEAVE L1 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `NSPEC` of the fibre.\n",
       "\n",
       "All column names are mde uppercase for consistency."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(read_fibre_table_nspec, name=\"read_fibre_table_nspec\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586594e2-4d40-43fc-b79d-3e707d4a0fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████| 120/120 [00:01<00:00, 84.95it/s] \n",
      "Creating Dataset... took 0.71 s. Size is 22.621 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  19%|██████▎                          | 23/120 [00:00<00:01, 78.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  27%|████████▊                        | 32/120 [00:00<00:01, 80.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  34%|███████████▎                     | 41/120 [00:00<00:00, 82.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  45%|██████████████▊                  | 54/120 [00:00<00:00, 97.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  53%|█████████████████▌               | 64/120 [00:00<00:00, 92.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  62%|████████████████████▎            | 74/120 [00:00<00:00, 88.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  72%|███████████████████████▉         | 87/120 [00:01<00:00, 94.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  82%|██████████████████████████▉      | 98/120 [00:01<00:00, 96.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  90%|████████████████████████████▊   | 108/120 [00:01<00:00, 97.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  99%|███████████████████████████████▋| 119/120 [00:01<00:00, 90.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files: 100%|████████████████████████████████| 120/120 [00:05<00:00, 21.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Dataset... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.72 s. Size is 22.621 Mb\n"
     ]
    }
   ],
   "source": [
    "raw_fibre_table = read_fibre_table(lr_raw_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35bc324-b67e-4fd9-846a-702d14830dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 24MB\n",
      "Dimensions:    (APS_ID: 1008, filename: 120)\n",
      "Coordinates:\n",
      "  * APS_ID     (APS_ID) int16 2kB 0 1 2 3 4 5 ... 1002 1003 1004 1005 1006 1007\n",
      "  * filename   (filename) object 960B 'r1003318' 'r1003313' ... 'r1004146'\n",
      "    RUN        (filename) int64 960B 1003318 1003313 1003314 ... 1004152 1004146\n",
      "    CAMERA     (filename) <U4 2kB 'BLUE' 'RED' 'BLUE' ... 'RED' 'BLUE' 'BLUE'\n",
      "    MJD        (filename) float64 960B 5.781e+04 5.781e+04 ... 5.803e+04\n",
      "    NIGHT      (filename) <U8 4kB '20170224' '20170224' ... '20170930'\n",
      "    OBID       (filename) int64 960B 3802 3802 3802 3802 ... 3936 3936 3936 3936\n",
      "Data variables: (12/36)\n",
      "    CNAME      (filename, APS_ID) object 968kB b'WVE_10011233+0259032' ... nan\n",
      "    FIBRERA    (filename, APS_ID) float64 968kB 150.3 nan nan ... nan nan nan\n",
      "    FIBREDEC   (filename, APS_ID) float64 968kB 2.984 nan nan ... nan nan nan\n",
      "    XPOSITION  (filename, APS_ID) float32 484kB -29.01 nan nan ... nan nan nan\n",
      "    YPOSITION  (filename, APS_ID) float32 484kB 3.104e+03 nan nan ... nan nan\n",
      "    STATUS     (filename, APS_ID) object 968kB b'A' nan nan nan ... nan nan nan\n",
      "    ...         ...\n",
      "    MAG_GG     (filename, APS_ID) float32 484kB 14.6 nan nan nan ... nan nan nan\n",
      "    EMAG_GG    (filename, APS_ID) float32 484kB 0.0 nan nan nan ... nan nan nan\n",
      "    MAG_BP     (filename, APS_ID) float32 484kB 14.95 nan nan ... nan nan nan\n",
      "    EMAG_BP    (filename, APS_ID) float32 484kB 0.003 nan nan ... nan nan nan\n",
      "    MAG_RP     (filename, APS_ID) float32 484kB 14.11 nan nan ... nan nan nan\n",
      "    EMAG_RP    (filename, APS_ID) float32 484kB 0.001 nan nan ... nan nan nan\n"
     ]
    }
   ],
   "source": [
    "print(raw_fibre_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b33f45-c84e-4444-ad00-c18ccb2e70d8",
   "metadata": {},
   "source": [
    "### GUIDINFO\n",
    "\n",
    "The GUIDINFO extension contains info about the guiding. Currently not sure of the best way to organise this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e19840-e0c6-4900-be75-52325cb1e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table.read(raw_hdus[\"GUIDINFO\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da3c4d-5f8d-41ac-b242-d9f86f97e61d",
   "metadata": {},
   "source": [
    "### METINFO\n",
    "\n",
    "The METINFO extension contains meteographical information. Currently not sure of the best way to organise this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b12054-d3d9-4b2f-997a-66fe87c23c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table.read(raw_hdus[\"METINFO\"], unit_parse_strict=\"silent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c9cc8-f2c3-4cf9-abcb-93170c8c7c23",
   "metadata": {},
   "source": [
    "## L1 files\n",
    "\n",
    "These contain lower-level processed data products. There are `single` files, which contain info for a single exposure, and `stack` files, which contain the same info for stacked exposures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc2a93-7295-4e9d-851a-3a9122b22258",
   "metadata": {},
   "source": [
    "We will read some simulated single files to show examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08c111-e1d2-4008-adbb-7545c22070b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 low-res L1 single files\n"
     ]
    }
   ],
   "source": [
    "lr_l1_single_files = get_lr_l1_single_files(date=\"2017*\")\n",
    "print(len(lr_l1_single_files), \"low-res L1 single files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2def5-d446-44f6-bc91-6a7c3850c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_hdus = fits.open(lr_l1_single_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439c18f-2469-44da-b319-5d9f433dc4de",
   "metadata": {},
   "source": [
    "WEAVE L1 single files contain seven extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3ff51-6770-4bd9-bf3a-6896aca06fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRIMARY', 'RED_DATA', 'RED_IVAR', 'RED_DATA_NOSS', 'RED_IVAR_NOSS', 'RED_SENSFUNC', 'FIBTABLE']\n"
     ]
    }
   ],
   "source": [
    "print([hdu.name for hdu in l1_hdus])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7a436a-635a-457f-8a83-6962f5d8cfb6",
   "metadata": {},
   "source": [
    "### PRIMARY\n",
    "\n",
    "The PRIMARY extension contains only a header with lots of information about the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dbfee3-a087-4899-ad2e-7e54828f7e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SIMPLE  =                    T / file does conform to FITS standard             \n",
       "BITPIX  =                    8 / number of bits per data pixel                  \n",
       "NAXIS   =                    0 / number of data axes                            \n",
       "EXTEND  =                    T / FITS dataset may contain extensions            \n",
       "COMMENT   FITS (Flexible Image Transport System) format is defined in 'Astronomy\n",
       "COMMENT   and Astrophysics', volume 376, page 359; bibcode: 2001A&A...376..359H \n",
       "COMMENT -------- Start of the CAMERA Packet -------                             \n",
       "RUN     =              1003317                                                  \n",
       "IRAFNAME= 'r1003317'           / redir r2840376.fit > r1003317                  \n",
       "DETECTOR= 'WVRED   '           / Selected by inference                          \n",
       "CCDSPEED= 'SLOW    '                                                            \n",
       "CCDXBIN =                    1                                                  \n",
       "CCDYBIN =                    1                                                  \n",
       "CCDSUM  = '1 1     '                                                            \n",
       "CCDTEMP =    180.2992022070757 / Randomly generated                             "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_hdus[\"PRIMARY\"].header[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff773a-a8ef-4313-b945-b720e2ffc573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████| 60/60 [00:00<00:00, 83.35it/s]\n",
      "Creating Dataset... took 1.14 s. Size is 0.379 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  42%|██████████████▏                   | 25/60 [00:00<00:00, 93.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  58%|███████████████████▊              | 35/60 [00:00<00:00, 87.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  78%|██████████████████████████▋       | 47/60 [00:00<00:00, 67.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  95%|████████████████████████████████▎ | 57/60 [00:00<00:00, 70.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files: 100%|██████████████████████████████████| 60/60 [00:00<00:00, 73.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Dataset... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 1.01 s. Size is 0.379 Mb\n"
     ]
    }
   ],
   "source": [
    "l1_primary_header = read_primary_header(lr_l1_single_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6060dd-58ab-4934-b263-41e0f064237a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 397kB\n",
      "Dimensions:   (filename: 60)\n",
      "Coordinates:\n",
      "  * filename  (filename) object 480B 'single_1003318' ... 'single_1004150'\n",
      "    RUN       (filename) int64 480B 1003318 1003317 1003320 ... 1004149 1004150\n",
      "    CAMERA    (filename) <U4 960B 'BLUE' 'RED' 'BLUE' ... 'BLUE' 'RED' 'BLUE'\n",
      "    OBID      (filename) int64 480B 3802 3802 3802 3802 ... 4407 3936 3936 3936\n",
      "    MJD       (filename) float64 480B 5.781e+04 5.781e+04 ... 5.803e+04\n",
      "    NIGHT     (filename) <U8 2kB '20170224' '20170224' ... '20170930' '20170930'\n",
      "Data variables: (12/447)\n",
      "    SIMPLE    (filename) bool 60B True True True True ... True True True True\n",
      "    BITPIX    (filename) int64 480B 8 8 8 8 8 8 8 8 8 8 ... 8 8 8 8 8 8 8 8 8 8\n",
      "    NAXIS     (filename) int64 480B 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0\n",
      "    EXTEND    (filename) bool 60B True True True True ... True True True True\n",
      "    IRAFNAME  (filename) <U8 2kB 'r1003318' 'r1003317' ... 'r1004149' 'r1004150'\n",
      "    DETECTOR  (filename) <U6 1kB 'WVBLUE' 'WVRED' 'WVBLUE' ... 'WVRED' 'WVBLUE'\n",
      "    ...        ...\n",
      "    NCOMB     (filename) int64 480B 1 1 1 1 1 1 1 1 1 1 ... 1 1 1 1 1 1 1 1 1 1\n",
      "    PROV0000  (filename) <U18 4kB 'single_1003318.fit' ... 'single_1004150.fit'\n",
      "    CHECKSUM  (filename) <U16 4kB 'hALkk7KihAKih7Ki' ... '9lIECkHE9kHECkHE'\n",
      "    DATASUM   (filename) <U1 240B '0' '0' '0' '0' '0' ... '0' '0' '0' '0' '0'\n",
      "    DELZPGRA  (filename) float64 480B 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
      "    CALDATE   (filename) int64 480B 20161225 20161225 ... 20161225 20161225\n"
     ]
    }
   ],
   "source": [
    "print(l1_primary_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb2040-4f4d-43c3-9fbb-a764d7bb346c",
   "metadata": {},
   "source": [
    "### DATA and SENSFUNC\n",
    "\n",
    "The RED/BLUE_DATA, RED/BLUE_IVAR, RED/BLUE_DATA_NOSS, RED/BLUE_IVAR_NOSS and RED_BLUE_SENSFUNC extensions contain the reduced binned spectra and their inverse variance, with and without sky subtraction, plus the sensitivity function. Implementation TBD, but I think it makes sense for all of these to be stored in a single Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc65e44-50bc-4988-8afd-ada44da3841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def _l1_data_reader(fn):\n",
    "    \"\"\"Read the data from a WEAVE L1 FITS file as a Dataset.\"\"\"\n",
    "    hdus = fits.open(fn)\n",
    "    camera = hdus[\"PRIMARY\"].header[\"CAMERA\"].replace(\"WEAVE\", \"\")\n",
    "    band = camera[0]\n",
    "    hdr = hdus[f\"{camera}_DATA\"].header\n",
    "    increment, zeropoint, size_wl, size_nspec = (\n",
    "        hdr[\"CD1_1\"],\n",
    "        hdr[\"CRVAL1\"],\n",
    "        hdr[\"NAXIS1\"],\n",
    "        hdr[\"NAXIS2\"],\n",
    "    )\n",
    "    wl = np.arange(0, size_wl) * increment + zeropoint\n",
    "    nspec = np.arange(1, size_nspec + 1)\n",
    "    coords = {\"NSPEC\": nspec, f\"LAMBDA_{band}\": wl}\n",
    "    dims = list(coords.keys())\n",
    "    arrays = {}\n",
    "    for ext in [\"DATA\", \"IVAR\", \"DATA_NOSS\", \"IVAR_NOSS\", \"SENSFUNC\"]:\n",
    "        name = f\"{camera}_{ext}\"\n",
    "        data = hdus[name].data\n",
    "        unit = hdus[name].header[\"BUNIT\"]\n",
    "        name = name.replace(\"DATA\", \"FLUX\")\n",
    "        arrays[name] = xr.Variable(dims, data, attrs={\"unit\": str(unit)})\n",
    "    return xr.Dataset(arrays, coords)\n",
    "\n",
    "\n",
    "read_l1_data = to_dataset(_l1_data_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff980e11-f79c-4031-bdb8-2ef8f69ff110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L375){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_l1_data\n",
       "\n",
       ">      read_l1_data (fn)\n",
       "\n",
       "Read the data from a WEAVE L1 FITS file as a Dataset."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L375){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_l1_data\n",
       "\n",
       ">      read_l1_data (fn)\n",
       "\n",
       "Read the data from a WEAVE L1 FITS file as a Dataset."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(read_l1_data, name=\"read_l1_data\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b8c59-ec7f-492d-b230-52739177cb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locating and converting where necessary: 100%|██████████| 60/60 [00:21<00:00,  2.75it/s]\n",
      "Reading netCDF files... took 3.47 s. Size is 27397.910 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:   8%|▊        | 5/60 [00:01<00:13,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  10%|▉        | 6/60 [00:01<00:11,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  15%|█▎       | 9/60 [00:02<00:09,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  17%|█▎      | 10/60 [00:02<00:11,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  18%|█▍      | 11/60 [00:02<00:10,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  22%|█▋      | 13/60 [00:02<00:07,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  23%|█▊      | 14/60 [00:03<00:09,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  25%|██      | 15/60 [00:03<00:08,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  27%|██▏     | 16/60 [00:03<00:08,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  28%|██▎     | 17/60 [00:03<00:07,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  32%|██▌     | 19/60 [00:04<00:11,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  33%|██▋     | 20/60 [00:04<00:11,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  35%|██▊     | 21/60 [00:05<00:11,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  38%|███     | 23/60 [00:05<00:07,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  40%|███▏    | 24/60 [00:06<00:17,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  42%|███▎    | 25/60 [00:07<00:14,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  45%|███▌    | 27/60 [00:07<00:12,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  47%|███▋    | 28/60 [00:08<00:13,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  48%|███▊    | 29/60 [00:08<00:12,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  50%|████    | 30/60 [00:09<00:12,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  52%|████▏   | 31/60 [00:09<00:09,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  57%|████▌   | 34/60 [00:10<00:09,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  58%|████▋   | 35/60 [00:10<00:09,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  65%|█████▏  | 39/60 [00:10<00:03,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  68%|█████▍  | 41/60 [00:11<00:03,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  72%|█████▋  | 43/60 [00:11<00:03,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  73%|█████▊  | 44/60 [00:11<00:02,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  77%|██████▏ | 46/60 [00:11<00:01,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  82%|██████▌ | 49/60 [00:11<00:01,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  85%|██████▊ | 51/60 [00:12<00:01,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  87%|██████▉ | 52/60 [00:12<00:01,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  92%|███████▎| 55/60 [00:13<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  95%|███████▌| 57/60 [00:13<00:00,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary: 100%|████████| 60/60 [00:13<00:00, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary: 100%|████████| 60/60 [00:13<00:00,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading netCDF files... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 3.22 s. Size is 27397.910 Mb\n"
     ]
    }
   ],
   "source": [
    "l1_data = read_l1_data(lr_l1_single_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8cda12-e9a4-4c5d-9ea5-eb23a9130a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 29GB\n",
      "Dimensions:         (NSPEC: 960, LAMBDA_B: 9649, filename: 60, LAMBDA_R: 15289)\n",
      "Coordinates:\n",
      "  * NSPEC           (NSPEC) int64 8kB 1 2 3 4 5 6 7 ... 955 956 957 958 959 960\n",
      "  * LAMBDA_B        (LAMBDA_B) float64 77kB 3.676e+03 3.676e+03 ... 6.088e+03\n",
      "  * filename        (filename) <U14 3kB 'single_1003322' ... 'single_1004149'\n",
      "    RUN             (filename) int64 480B dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "    CAMERA          (filename) <U4 960B dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "    MJD             (filename) float64 480B dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "    NIGHT           (filename) <U8 2kB dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "    OBID            (filename) int64 480B dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "  * LAMBDA_R        (LAMBDA_R) float64 122kB 5.772e+03 5.772e+03 ... 9.594e+03\n",
      "Data variables:\n",
      "    BLUE_FLUX       (filename, NSPEC, LAMBDA_B) float32 2GB dask.array<chunksize=(1, 960, 9649), meta=np.ndarray>\n",
      "    BLUE_IVAR       (filename, NSPEC, LAMBDA_B) float32 2GB dask.array<chunksize=(1, 960, 9649), meta=np.ndarray>\n",
      "    BLUE_FLUX_NOSS  (filename, NSPEC, LAMBDA_B) float32 2GB dask.array<chunksize=(1, 960, 9649), meta=np.ndarray>\n",
      "    BLUE_IVAR_NOSS  (filename, NSPEC, LAMBDA_B) float32 2GB dask.array<chunksize=(1, 960, 9649), meta=np.ndarray>\n",
      "    BLUE_SENSFUNC   (filename, NSPEC, LAMBDA_B) float32 2GB dask.array<chunksize=(1, 960, 9649), meta=np.ndarray>\n",
      "    RED_FLUX        (filename, NSPEC, LAMBDA_R) float32 4GB dask.array<chunksize=(6, 960, 15289), meta=np.ndarray>\n",
      "    RED_IVAR        (filename, NSPEC, LAMBDA_R) float32 4GB dask.array<chunksize=(6, 960, 15289), meta=np.ndarray>\n",
      "    RED_FLUX_NOSS   (filename, NSPEC, LAMBDA_R) float32 4GB dask.array<chunksize=(6, 960, 15289), meta=np.ndarray>\n",
      "    RED_IVAR_NOSS   (filename, NSPEC, LAMBDA_R) float32 4GB dask.array<chunksize=(6, 960, 15289), meta=np.ndarray>\n",
      "    RED_SENSFUNC    (filename, NSPEC, LAMBDA_R) float32 4GB dask.array<chunksize=(6, 960, 15289), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "print(l1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74345508-a5ec-478e-b890-dffcc43baaa1",
   "metadata": {},
   "source": [
    "It is a good idea to close a `Dataset` when you are finished with it, as otherwise other processes may not be able to access the same underlying files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae8d5e-e3f0-4e9f-a1b9-ca7d169a4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be383c0d-647c-4650-af52-100373ac3353",
   "metadata": {},
   "source": [
    "### FIBTABLE\n",
    "\n",
    "The FIBTABLE extension contains information about the fibre allocations, plus some basic measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42651204-d8e3-4ba1-bd8f-ed6d7784369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table.read(l1_hdus[\"FIBTABLE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3692a-1673-4cf9-bdb3-8d06dd7fe9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████| 60/60 [00:01<00:00, 53.93it/s]\n",
      "Creating Dataset... took 0.53 s. Size is 18.850 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  28%|█████████▋                        | 17/60 [00:00<00:00, 44.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  42%|██████████████▏                   | 25/60 [00:00<00:00, 51.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  55%|██████████████████▋               | 33/60 [00:00<00:00, 57.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  70%|███████████████████████▊          | 42/60 [00:00<00:00, 66.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  85%|████████████████████████████▉     | 51/60 [00:00<00:00, 67.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  98%|█████████████████████████████████▍| 59/60 [00:01<00:00, 70.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files: 100%|██████████████████████████████████| 60/60 [00:01<00:00, 58.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Dataset... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.58 s. Size is 18.850 Mb\n"
     ]
    }
   ],
   "source": [
    "l1_fibre_table = read_fibre_table(lr_l1_single_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64ee55-aaa5-490f-903d-99ff6a8d7edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 20MB\n",
      "Dimensions:       (APS_ID: 1004, filename: 60)\n",
      "Coordinates:\n",
      "  * APS_ID        (APS_ID) int16 2kB 1 2 3 4 5 6 ... 1003 1004 1005 1006 1007\n",
      "  * filename      (filename) object 480B 'single_1003318' ... 'single_1004150'\n",
      "    RUN           (filename) int64 480B 1003318 1003317 ... 1004149 1004150\n",
      "    CAMERA        (filename) <U4 960B 'BLUE' 'RED' 'RED' ... 'RED' 'RED' 'BLUE'\n",
      "    MJD           (filename) float64 480B 5.781e+04 5.781e+04 ... 5.803e+04\n",
      "    NIGHT         (filename) <U8 2kB '20170224' '20170224' ... '20170930'\n",
      "    OBID          (filename) int64 480B 3802 3802 3802 3802 ... 4407 3936 3936\n",
      "Data variables: (12/59)\n",
      "    NSPEC         (filename, APS_ID) float32 241kB nan nan nan ... nan nan nan\n",
      "    CNAME         (filename, APS_ID) object 482kB nan nan nan ... nan nan nan\n",
      "    FIBRERA       (filename, APS_ID) float64 482kB nan nan nan ... nan nan nan\n",
      "    FIBREDEC      (filename, APS_ID) float64 482kB nan nan nan ... nan nan nan\n",
      "    XPOSITION     (filename, APS_ID) float32 241kB nan nan nan ... nan nan nan\n",
      "    YPOSITION     (filename, APS_ID) float32 241kB nan nan nan ... nan nan nan\n",
      "    ...            ...\n",
      "    MEANFLUX_G    (filename, APS_ID) float32 241kB nan nan nan ... nan nan nan\n",
      "    MEANFLUX_R    (filename, APS_ID) float32 241kB nan nan nan ... nan nan nan\n",
      "    MEANFLUX_I    (filename, APS_ID) float32 241kB nan nan nan ... nan nan nan\n",
      "    MEANFLUX_GG   (filename, APS_ID) float32 241kB nan nan nan ... nan nan nan\n",
      "    MEANFLUX_BP   (filename, APS_ID) float32 241kB nan nan nan ... nan nan nan\n",
      "    MEANFLUX_RP   (filename, APS_ID) float32 241kB nan nan nan ... nan nan nan\n"
     ]
    }
   ],
   "source": [
    "print(l1_fibre_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b74459-0c05-4366-8e74-2b3abf2794d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████| 60/60 [00:00<00:00, 65.14it/s]\n",
      "Creating Dataset... took 0.41 s. Size is 18.024 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  28%|█████████▋                        | 17/60 [00:00<00:01, 37.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  42%|██████████████▏                   | 25/60 [00:00<00:00, 47.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  55%|██████████████████▋               | 33/60 [00:00<00:00, 54.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  68%|███████████████████████▏          | 41/60 [00:00<00:00, 61.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  82%|███████████████████████████▊      | 49/60 [00:01<00:00, 66.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files:  95%|████████████████████████████████▎ | 57/60 [00:01<00:00, 69.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading files: 100%|██████████████████████████████████| 60/60 [00:01<00:00, 51.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Dataset... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.64 s. Size is 18.024 Mb\n"
     ]
    }
   ],
   "source": [
    "l1_fibre_table_nspec = read_fibre_table_nspec(lr_l1_single_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1350619-c5d1-4623-a0ef-63107ba80e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 19MB\n",
      "Dimensions:       (NSPEC: 960, filename: 60)\n",
      "Coordinates:\n",
      "  * NSPEC         (NSPEC) int16 2kB 1 2 3 4 5 6 7 ... 955 956 957 958 959 960\n",
      "  * filename      (filename) object 480B 'single_1003317' ... 'single_1004150'\n",
      "    RUN           (filename) int64 480B 1003317 1003319 ... 1004149 1004150\n",
      "    CAMERA        (filename) <U4 960B 'RED' 'RED' 'BLUE' ... 'BLUE' 'RED' 'BLUE'\n",
      "    MJD           (filename) float64 480B 5.781e+04 5.781e+04 ... 5.803e+04\n",
      "    NIGHT         (filename) <U8 2kB '20170224' '20170224' ... '20170930'\n",
      "    OBID          (filename) int64 480B 3802 3802 3802 3802 ... 3936 3936 3936\n",
      "Data variables: (12/59)\n",
      "    CNAME         (filename, NSPEC) object 461kB b'WVE_10005104+0244131' ... nan\n",
      "    FIBRERA       (filename, NSPEC) float64 461kB 150.2 150.2 150.4 ... nan nan\n",
      "    FIBREDEC      (filename, NSPEC) float64 461kB 2.737 2.653 2.6 ... nan nan\n",
      "    XPOSITION     (filename, NSPEC) float32 230kB -19.73 -18.88 ... nan nan\n",
      "    YPOSITION     (filename, NSPEC) float32 230kB 123.1 105.9 95.0 ... nan nan\n",
      "    STATUS        (filename, NSPEC) object 461kB b'A' b'A' b'A' ... nan nan nan\n",
      "    ...            ...\n",
      "    MEANFLUX_R    (filename, NSPEC) float32 230kB 4.267 8.351 21.52 ... nan nan\n",
      "    MEANFLUX_I    (filename, NSPEC) float32 230kB 6.963 12.02 30.68 ... nan nan\n",
      "    MEANFLUX_GG   (filename, NSPEC) float32 230kB 5.622 9.648 24.76 ... nan nan\n",
      "    MEANFLUX_BP   (filename, NSPEC) float32 230kB nan nan nan ... nan nan nan\n",
      "    MEANFLUX_RP   (filename, NSPEC) float32 230kB 5.621 9.648 24.76 ... nan nan\n",
      "    APS_ID        (filename, NSPEC) float32 230kB 1.007e+03 1.006e+03 ... nan\n"
     ]
    }
   ],
   "source": [
    "print(l1_fibre_table_nspec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce72ca35-c8d4-4896-a481-729518d186c9",
   "metadata": {},
   "source": [
    "## L2 files\n",
    "\n",
    "These contain higher-level processed data products. There are `single` files, which contain meaurements on a single exposure, and `stack` files, which contain the same measurements on stacked exposures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9437282-e350-4401-a815-a071838a7df9",
   "metadata": {},
   "source": [
    "We will read some simulated stack files to show examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b3be4-7f74-465a-8b69-c5041f61679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 low-res L2 stack files\n"
     ]
    }
   ],
   "source": [
    "lr_l2_stack_files = get_lr_l2_stack_files(date=\"2017*\")\n",
    "print(len(lr_l2_stack_files), \"low-res L2 stack files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf8b58-3990-4a46-a68e-5f5bc31027f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_hdus = fits.open(lr_l2_stack_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d7145",
   "metadata": {},
   "source": [
    "WEAVE L2 stack files contain six extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb6b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRIMARY', 'CLASS_TABLE', 'STAR_TABLE', 'GALAXY_TABLE', 'CLASS_SPEC', 'STAR_SPEC', 'GALAXY_SPEC']\n"
     ]
    }
   ],
   "source": [
    "print([hdu.name for hdu in l2_hdus])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa822d2c",
   "metadata": {},
   "source": [
    "### PRIMARY\n",
    "\n",
    "The PRIMARY extension contains only a header with some basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a901b78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SIMPLE  =                    T / conforms to FITS standard                      \n",
       "BITPIX  =                    8 / array data type                                \n",
       "NAXIS   =                    0 / number of array dimensions                     \n",
       "EXTEND  =                    T                                                  \n",
       "L1_REF_0= 'stack_1003318.fit'  / L1 reference file                              \n",
       "L1_REF_1= 'stack_1003317.fit'  / L1 reference file                              \n",
       "L1_REF_2= '' / L1 reference file                                                \n",
       "DATE-OBS= '20170224'           / L1: OBS-DATE                                   \n",
       "OBSMODE = 'MOS     '           / L1: OBSMODE                                    \n",
       "RES-OBS = 'LR      '           / L2: RES-DATE                                   \n",
       "OBID    = '3802    '           / L1: OBID                                       \n",
       "CHECKSUM= '97WQA7WO97WOA7WO'   / HDU checksum updated 2022-02-08T02:11:32       \n",
       "DATASUM = '0       '           / data unit checksum updated 2022-02-08T02:11:32 "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_hdus[\"PRIMARY\"].header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2df3d3-2c02-4f7d-a4d2-494a0551f660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████| 6/6 [00:00<00:00, 102.97it/s]\n",
      "Creating Dataset... took 0.01 s. Size is 0.002 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dataset... took 0.01 s. Size is 0.002 Mb\n"
     ]
    }
   ],
   "source": [
    "l2_primary_header = read_primary_header(lr_l2_stack_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dad710-e327-4a12-96f2-9263947790ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 2kB\n",
      "Dimensions:   (filename: 6)\n",
      "Coordinates:\n",
      "  * filename  (filename) object 48B 'stack_1003354__stack_1003353_APS' ... 's...\n",
      "    OBID      (filename) <U4 96B '3653' '3756' '3803' '3900' '3806' '3802'\n",
      "Data variables:\n",
      "    SIMPLE    (filename) bool 6B True True True True True True\n",
      "    BITPIX    (filename) int64 48B 8 8 8 8 8 8\n",
      "    NAXIS     (filename) int64 48B 0 0 0 0 0 0\n",
      "    EXTEND    (filename) bool 6B True True True True True True\n",
      "    L1_REF_0  (filename) <U17 408B 'stack_1003354.fit' ... 'stack_1003318.fit'\n",
      "    L1_REF_1  (filename) <U17 408B 'stack_1003353.fit' ... 'stack_1003317.fit'\n",
      "    L1_REF_2  (filename) <U1 24B '' '' '' '' '' ''\n",
      "    DATE-OBS  (filename) <U8 192B '20170225' '20170227' ... '20170224'\n",
      "    OBSMODE   (filename) <U3 72B 'MOS' 'MOS' 'MOS' 'MOS' 'MOS' 'MOS'\n",
      "    RES-OBS   (filename) <U2 48B 'LR' 'LR' 'LR' 'LR' 'LR' 'LR'\n",
      "    CHECKSUM  (filename) <U16 384B '97WNC7TL97TLC7TL' ... '97WQA7WO97WOA7WO'\n",
      "    DATASUM   (filename) <U1 24B '0' '0' '0' '0' '0' '0'\n"
     ]
    }
   ],
   "source": [
    "print(l2_primary_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a381612-1383-4641-9e8d-1c5c2fd6bfed",
   "metadata": {},
   "source": [
    "### CLASS_TABLE\n",
    "\n",
    "The CLASS_TABLE extension contains information from matching various templates to the spectra. The redshift and class of the best fitting template are given, as well as the full cross-correlation results of each template as a function of redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad1999-309c-431c-aef0-54ddd9000b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table.read(l2_hdus[\"CLASS_TABLE\"]) # contains multidimensional columns, so not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47fe98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def _class_table_reader(fn):\n",
    "    \"\"\"Read the CLASS_TABLE from a WEAVE L2 FITS file as a Dataset.\n",
    "\n",
    "    All quantities are indexed by the `APS_ID` of the fibre.\n",
    "    Chi-square values `CZZ_CHI2_*` for each template are further indexed by redshift `CZZ_*`.\n",
    "    Coefficients `COEFF` and indexed by integers `I_COEFF`.\n",
    "    \"\"\"\n",
    "    cols = _read_fits_columns(fn, \"CLASS_TABLE\")\n",
    "    if not cols:\n",
    "        return None\n",
    "    coords = dict(APS_ID=cols.pop(\"APS_ID\"))\n",
    "    # convert CZZ columns to coordinates\n",
    "    for c in list(cols):\n",
    "        if c.startswith(\"CZZ\") and \"CHI2\" not in c:\n",
    "            czz_all = cols.pop(c)\n",
    "            czz = czz_all[0]\n",
    "            assert (czz == czz_all).all()\n",
    "            coords[c] = czz\n",
    "    for c in cols:\n",
    "        dims = [\"APS_ID\"]\n",
    "        if c.startswith(\"CZZ\"):\n",
    "            dims += [c.replace(\"_CHI2\", \"\")]\n",
    "        elif c == \"COEFF\":\n",
    "            dims += [\"I_COEFF\"]\n",
    "        cols[c] = xr.Variable(dims, cols[c], attrs={\"unit\": str(cols[c].unit)})\n",
    "    return xr.Dataset(cols, coords)\n",
    "\n",
    "\n",
    "read_class_table = to_dataset(_class_table_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206d413-692f-452f-a78c-2cb012714f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L404){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_class_table\n",
       "\n",
       ">      read_class_table (fn)\n",
       "\n",
       "Read the CLASS_TABLE from a WEAVE L2 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "Chi-square values `CZZ_CHI2_*` for each template are further indexed by redshift `CZZ_*`.\n",
       "Coefficients `COEFF` and indexed by integers `I_COEFF`."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L404){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_class_table\n",
       "\n",
       ">      read_class_table (fn)\n",
       "\n",
       "Read the CLASS_TABLE from a WEAVE L2 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "Chi-square values `CZZ_CHI2_*` for each template are further indexed by redshift `CZZ_*`.\n",
       "Coefficients `COEFF` and indexed by integers `I_COEFF`."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(read_class_table, name=\"read_class_table\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590c27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:01<00:00,  3.19it/s]\n",
      "Reading netCDF files... took 1.19 s. Size is 178.858 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  67%|██████▋   | 4/6 [00:01<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:01<00:00,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading netCDF files... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 1.14 s. Size is 178.858 Mb\n"
     ]
    }
   ],
   "source": [
    "class_table = read_class_table(lr_l2_stack_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafeb84-144a-4d1a-8dbf-10a9a28d1d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 188MB\n",
      "Dimensions:           (APS_ID: 995, CZZ_GALAXY: 1446, CZZ_QSO: 1648,\n",
      "                       CZZ_STAR_A: 101, CZZ_STAR_B: 101, CZZ_STAR_CV: 101,\n",
      "                       CZZ_STAR_F: 101, CZZ_STAR_G: 101, CZZ_STAR_K: 101,\n",
      "                       CZZ_STAR_M: 101, CZZ_STAR_WD: 101, filename: 6,\n",
      "                       I_COEFF: 10)\n",
      "Coordinates: (12/13)\n",
      "  * APS_ID            (APS_ID) int64 8kB 1 2 3 4 5 ... 1003 1004 1005 1006 1007\n",
      "  * CZZ_GALAXY        (CZZ_GALAXY) float64 12kB -0.005 -0.004312 ... 1.698 1.7\n",
      "  * CZZ_QSO           (CZZ_QSO) float64 13kB 0.05 0.05121 ... 5.985 5.993\n",
      "  * CZZ_STAR_A        (CZZ_STAR_A) float64 808B -0.002 -0.00196 ... 0.002\n",
      "  * CZZ_STAR_B        (CZZ_STAR_B) float64 808B -0.002 -0.00196 ... 0.002\n",
      "  * CZZ_STAR_CV       (CZZ_STAR_CV) float64 808B -0.002 -0.00196 ... 0.002\n",
      "    ...                ...\n",
      "  * CZZ_STAR_G        (CZZ_STAR_G) float64 808B -0.002 -0.00196 ... 0.002\n",
      "  * CZZ_STAR_K        (CZZ_STAR_K) float64 808B -0.002 -0.00196 ... 0.002\n",
      "  * CZZ_STAR_M        (CZZ_STAR_M) float64 808B -0.002 -0.00196 ... 0.002\n",
      "  * CZZ_STAR_WD       (CZZ_STAR_WD) float64 808B -0.002 -0.00196 ... 0.002\n",
      "  * filename          (filename) <U32 768B 'stack_1003330__stack_1003329_APS'...\n",
      "    OBID              (filename) <U4 96B dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "Dimensions without coordinates: I_COEFF\n",
      "Data variables: (12/25)\n",
      "    TARGID            (filename, APS_ID) object 48kB dask.array<chunksize=(1, 995), meta=np.ndarray>\n",
      "    CNAME             (filename, APS_ID) object 48kB dask.array<chunksize=(1, 995), meta=np.ndarray>\n",
      "    Z                 (filename, APS_ID) float64 48kB dask.array<chunksize=(1, 995), meta=np.ndarray>\n",
      "    ZERR              (filename, APS_ID) float64 48kB dask.array<chunksize=(1, 995), meta=np.ndarray>\n",
      "    ZWARN             (filename, APS_ID) float64 48kB dask.array<chunksize=(1, 995), meta=np.ndarray>\n",
      "    CLASS             (filename, APS_ID) object 48kB dask.array<chunksize=(1, 995), meta=np.ndarray>\n",
      "    ...                ...\n",
      "    CZZ_CHI2_STAR_CV  (filename, APS_ID, CZZ_STAR_CV) float64 5MB dask.array<chunksize=(1, 995, 101), meta=np.ndarray>\n",
      "    CZZ_CHI2_STAR_F   (filename, APS_ID, CZZ_STAR_F) float64 5MB dask.array<chunksize=(1, 995, 101), meta=np.ndarray>\n",
      "    CZZ_CHI2_STAR_G   (filename, APS_ID, CZZ_STAR_G) float64 5MB dask.array<chunksize=(1, 995, 101), meta=np.ndarray>\n",
      "    CZZ_CHI2_STAR_K   (filename, APS_ID, CZZ_STAR_K) float64 5MB dask.array<chunksize=(1, 995, 101), meta=np.ndarray>\n",
      "    CZZ_CHI2_STAR_M   (filename, APS_ID, CZZ_STAR_M) float64 5MB dask.array<chunksize=(1, 995, 101), meta=np.ndarray>\n",
      "    CZZ_CHI2_STAR_WD  (filename, APS_ID, CZZ_STAR_WD) float64 5MB dask.array<chunksize=(1, 995, 101), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "print(class_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f61a4-8ab7-4b33-966e-6b37d69637ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_table.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a8fdad-f1bf-4daa-93a4-55471a96cebf",
   "metadata": {},
   "source": [
    "### STAR_TABLE\n",
    "\n",
    "The STAR_TABLE extension contains measurements of stellar parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48410e65-cff6-4e82-b150-6835c534162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table.read(l2_hdus[\"STAR_TABLE\"])  # contains multidimensional columns, so not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578ab2c-add0-4cc7-a780-94b252d52dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def _star_table_reader(fn):\n",
    "    \"\"\"Read the STAR_TABLE from a WEAVE L2 FITS file as a Dataset.\n",
    "\n",
    "    All quantities are indexed by the `APS_ID` of the fibre.\n",
    "    The covariance matrix `COVAR` is additionally indexed by `I_COVAR`, `J_COVAR`.\n",
    "    The elements `ELEM` and `ELEM_ERR` are additionally indexed by `I_ELEM`.\n",
    "    \"\"\"\n",
    "    cols = _read_fits_columns(fn, \"STAR_TABLE\")\n",
    "    if not cols:\n",
    "        return None\n",
    "    coords = dict(APS_ID=cols.pop(\"APS_ID\"))\n",
    "    coords[\"I_COVAR\"] = coords[\"J_COVAR\"] = [\"TEFF\", \"LOGG\", \"FEH\", \"ALPHA\", \"MICRO\"]\n",
    "    for c in cols:\n",
    "        dims = [\"APS_ID\"]\n",
    "        if c == \"COVAR\":\n",
    "            dims += [\"I_COVAR\", \"J_COVAR\"]\n",
    "        elif \"ELEM\" in c:\n",
    "            dims += [\"I_ELEM\"]\n",
    "        cols[c] = xr.Variable(dims, cols[c], attrs={\"unit\": str(cols[c].unit)})\n",
    "    return xr.Dataset(cols, coords)\n",
    "\n",
    "\n",
    "read_star_table = to_dataset(_star_table_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23faebb5-1f75-43a4-b337-72f4387b2162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L435){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_star_table\n",
       "\n",
       ">      read_star_table (fn)\n",
       "\n",
       "Read the STAR_TABLE from a WEAVE L2 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "The covariance matrix `COVAR` is additionally indexed by `I_COVAR`, `J_COVAR`.\n",
       "The elements `ELEM` and `ELEM_ERR` are additionally indexed by `I_ELEM`."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L435){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_star_table\n",
       "\n",
       ">      read_star_table (fn)\n",
       "\n",
       "Read the STAR_TABLE from a WEAVE L2 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "The covariance matrix `COVAR` is additionally indexed by `I_COVAR`, `J_COVAR`.\n",
       "The elements `ELEM` and `ELEM_ERR` are additionally indexed by `I_ELEM`."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(read_star_table, name=\"read_star_table\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c52e8-a5d5-4e4d-af3e-cecf8aa1838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:00<00:00,  6.69it/s]\n",
      "Reading netCDF files... took 1.02 s. Size is 0.065 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading netCDF files... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 1.08 s. Size is 0.065 Mb\n"
     ]
    }
   ],
   "source": [
    "star_table = read_star_table(lr_l2_stack_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b009dec-529b-4fc1-a366-1e5738d73203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 69kB\n",
      "Dimensions:        (APS_ID: 24, I_COVAR: 5, J_COVAR: 5, filename: 6, I_ELEM: 2)\n",
      "Coordinates:\n",
      "  * APS_ID         (APS_ID) int64 192B 194 222 283 304 329 ... 922 946 948 992\n",
      "  * I_COVAR        (I_COVAR) <U5 100B 'TEFF' 'LOGG' 'FEH' 'ALPHA' 'MICRO'\n",
      "  * J_COVAR        (J_COVAR) <U5 100B 'TEFF' 'LOGG' 'FEH' 'ALPHA' 'MICRO'\n",
      "  * filename       (filename) <U32 768B 'stack_1003426__stack_1003425_APS' .....\n",
      "    OBID           (filename) <U4 96B dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "Dimensions without coordinates: I_ELEM\n",
      "Data variables: (12/33)\n",
      "    TARGID         (filename, APS_ID) object 1kB dask.array<chunksize=(1, 24), meta=np.ndarray>\n",
      "    CNAME          (filename, APS_ID) object 1kB dask.array<chunksize=(1, 24), meta=np.ndarray>\n",
      "    VRAD           (filename, APS_ID) float64 1kB dask.array<chunksize=(1, 24), meta=np.ndarray>\n",
      "    VRAD_ERR       (filename, APS_ID) float64 1kB dask.array<chunksize=(1, 24), meta=np.ndarray>\n",
      "    SKEWNESS_RVS   (filename, APS_ID) float64 1kB dask.array<chunksize=(1, 24), meta=np.ndarray>\n",
      "    KURTOSIS_RVS   (filename, APS_ID) float64 1kB dask.array<chunksize=(1, 24), meta=np.ndarray>\n",
      "    ...             ...\n",
      "    COVAR          (filename, APS_ID, I_COVAR, J_COVAR) float64 29kB dask.array<chunksize=(1, 24, 5, 5), meta=np.ndarray>\n",
      "    ELEM           (filename, APS_ID, I_ELEM) float64 2kB dask.array<chunksize=(1, 24, 2), meta=np.ndarray>\n",
      "    ELEM_ERR       (filename, APS_ID, I_ELEM) float64 2kB dask.array<chunksize=(1, 24, 2), meta=np.ndarray>\n",
      "    SNR_FR         (filename, APS_ID) float64 1kB dask.array<chunksize=(1, 24), meta=np.ndarray>\n",
      "    CHISQ_TOT      (filename, APS_ID) float64 1kB dask.array<chunksize=(1, 24), meta=np.ndarray>\n",
      "    FLAG_FR        (filename, APS_ID) float64 1kB dask.array<chunksize=(1, 24), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "print(star_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61c4be-a72b-4d21-bad5-e2880fe63bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_table.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ab63e5-9d0a-4948-b1ec-cf474f4daa06",
   "metadata": {},
   "source": [
    "### GALAXY_TABLE\n",
    "\n",
    "The GALAXY_TABLE extension contains measurements of galaxy parameters, including Hubble-flow corrected redshifts, stellar kinematics, line fits and indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15246746-54a3-4cdb-a1d8-7a65e556d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table.read(l2_hdus[\"GALAXY_TABLE\"], unit_parse_strict=\"silent\")  # contains multidimensional columns, so not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def _not_line_col(c):\n",
    "    \"\"\"Identify columns that do not contain line measurements.\"\"\"\n",
    "    c = c.replace(\"ERR_\", \"\")\n",
    "    for n in [\"EBMV0\", \"EBMV1\", \"FLUX\", \"AMPL\", \"Z\", \"SIGMA\", \"AON\", \"FWHM\"]:\n",
    "        if c.startswith(n + \"_\"):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def _process_line_quantities(cols, lines):\n",
    "    \"\"\"Process line quantities.\n",
    "\n",
    "    Quantities with multiple elements are split into separate columns.\n",
    "\n",
    "    The supplied `cols` dictionary is modified in-place.\n",
    "    \"\"\"\n",
    "    line_quantities = []\n",
    "    for c in list(cols):\n",
    "        if c.endswith(lines[0]):\n",
    "            qty = c.replace(\"_\" + lines[0], \"\")\n",
    "            ndim = cols[c].ndim\n",
    "            if ndim == 1:\n",
    "                line_quantities.append(qty)\n",
    "            elif ndim == 2:\n",
    "                nq = cols[c].shape[1]\n",
    "                for i in range(nq):\n",
    "                    line_quantities.append(f\"{qty}{i}\")\n",
    "                for line in lines:\n",
    "                    oldcol = cols.pop(f\"{qty}_{line}\")\n",
    "                    for i in range(nq):\n",
    "                        cols[f\"{qty}{i}_{line}\"] = oldcol[:, i]\n",
    "    return line_quantities\n",
    "\n",
    "\n",
    "def _galaxy_table_reader(fn):\n",
    "    \"\"\"Read the GALAXY_TABLE from a WEAVE L2 FITS file as a Dataset.\n",
    "\n",
    "    All quantities are indexed by the `APS_ID` of the fibre.\n",
    "    The line measurements are additionally indexed by the measurement quantity `QTY`\n",
    "    and the line name `LINE`.\n",
    "    The index measurements are additionally indexed by the index name `INDEX`.\n",
    "    \"\"\"\n",
    "    # TODO: add units where missing\n",
    "    cols = _read_fits_columns(fn, \"GALAXY_TABLE\")\n",
    "    if not cols:\n",
    "        return None\n",
    "    coords = dict(APS_ID=cols.pop(\"APS_ID\"))\n",
    "    coords[\"LINE\"] = [c.replace(\"FLUX_\", \"\") for c in cols if c.startswith(\"FLUX\")]\n",
    "    coords[\"QTY\"] = _process_line_quantities(cols, coords[\"LINE\"])\n",
    "    coords[\"INDEX\"] = [c for i, c in enumerate(cols) if (i > 100 and _not_line_col(c))]\n",
    "    line_cols = [\n",
    "        [cols.pop(f\"{qty}_{line}\") for line in coords[\"LINE\"]] for qty in coords[\"QTY\"]\n",
    "    ]\n",
    "    index_cols = [cols.pop(idx) for idx in coords[\"INDEX\"]]\n",
    "    out_cols = {}\n",
    "    for i, c in enumerate(cols):\n",
    "        dims = [\"APS_ID\"]\n",
    "        out_cols[c] = xr.Variable(dims, cols[c], attrs={\"unit\": str(cols[c].unit)})\n",
    "    out_cols[\"LINES\"] = xr.Variable([\"QTY\", \"LINE\", \"APS_ID\"], line_cols)\n",
    "    out_cols[\"INDICES\"] = xr.Variable([\"INDEX\", \"APS_ID\"], index_cols)\n",
    "    return xr.Dataset(out_cols, coords)\n",
    "\n",
    "\n",
    "read_galaxy_table = to_dataset(_galaxy_table_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f985541f-b4db-4272-98ff-125c5c6ae6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L494){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_galaxy_table\n",
       "\n",
       ">      read_galaxy_table (fn)\n",
       "\n",
       "Read the GALAXY_TABLE from a WEAVE L2 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "The line measurements are additionally indexed by the measurement quantity `QTY`\n",
       "and the line name `LINE`.\n",
       "The index measurements are additionally indexed by the index name `INDEX`."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L494){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_galaxy_table\n",
       "\n",
       ">      read_galaxy_table (fn)\n",
       "\n",
       "Read the GALAXY_TABLE from a WEAVE L2 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "The line measurements are additionally indexed by the measurement quantity `QTY`\n",
       "and the line name `LINE`.\n",
       "The index measurements are additionally indexed by the index name `INDEX`."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(read_galaxy_table, name=\"read_galaxy_table\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ea6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:01<00:00,  4.63it/s]\n",
      "Reading netCDF files... took 0.95 s. Size is 27.393 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:01<00:00,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:01<00:00,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading netCDF files... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 1.30 s. Size is 27.393 Mb\n"
     ]
    }
   ],
   "source": [
    "galaxy_table = read_galaxy_table(lr_l2_stack_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194871bb-eb0d-47ce-9476-364611a35939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 29MB\n",
      "Dimensions:         (APS_ID: 994, LINE: 22, QTY: 13, INDEX: 292, filename: 6)\n",
      "Coordinates:\n",
      "  * APS_ID          (APS_ID) int32 4kB 1 2 3 4 5 6 ... 1003 1004 1005 1006 1007\n",
      "  * LINE            (LINE) <U15 1kB 'HeII_3203.15' ... '[ArIII]_7135.67'\n",
      "  * QTY             (QTY) <U9 468B 'FLUX' 'AMPL' 'Z' ... 'ERR_EBMV0' 'ERR_EBMV1'\n",
      "  * INDEX           (INDEX) <U16 19kB 'BL1719' 'ERR_BL1719' ... 'ERR_MgI2.28'\n",
      "  * filename        (filename) <U32 768B 'stack_1003438__stack_1003437_APS' ....\n",
      "    OBID            (filename) <U4 96B dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "Data variables: (12/26)\n",
      "    TARGID          (filename, APS_ID) object 48kB dask.array<chunksize=(1, 994), meta=np.ndarray>\n",
      "    CNAME           (filename, APS_ID) object 48kB dask.array<chunksize=(1, 994), meta=np.ndarray>\n",
      "    ZCORR           (filename, APS_ID) float64 48kB dask.array<chunksize=(1, 994), meta=np.ndarray>\n",
      "    V               (filename, APS_ID) float64 48kB dask.array<chunksize=(1, 994), meta=np.ndarray>\n",
      "    SIGMA           (filename, APS_ID) float64 48kB dask.array<chunksize=(1, 994), meta=np.ndarray>\n",
      "    H3              (filename, APS_ID) float64 48kB dask.array<chunksize=(1, 994), meta=np.ndarray>\n",
      "    ...              ...\n",
      "    FORM_ERR_H4     (filename, APS_ID) float64 48kB dask.array<chunksize=(1, 994), meta=np.ndarray>\n",
      "    FORM_ERR_H5     (filename, APS_ID) float64 48kB dask.array<chunksize=(1, 994), meta=np.ndarray>\n",
      "    FORM_ERR_H6     (filename, APS_ID) float64 48kB dask.array<chunksize=(1, 994), meta=np.ndarray>\n",
      "    FWHM_FLAG       (filename, APS_ID) float32 24kB dask.array<chunksize=(1, 994), meta=np.ndarray>\n",
      "    LINES           (filename, QTY, LINE, APS_ID) float64 14MB dask.array<chunksize=(1, 13, 22, 994), meta=np.ndarray>\n",
      "    INDICES         (filename, INDEX, APS_ID) float64 14MB dask.array<chunksize=(1, 292, 994), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "print(galaxy_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1085c7dc-736d-475c-97e6-16f7a7613eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy_table.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0705f9e6-446b-4dc0-b1c2-e8ffd9e307c2",
   "metadata": {},
   "source": [
    "### CLASS_SPEC\n",
    "\n",
    "The CLASS_SPEC extension contains spectra and model fits, performed separately for the red and blue arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636b432-232a-4f27-a766-4f4b9d76ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table.read(l2_hdus[\"CLASS_SPEC\"], unit_parse_strict=\"silent\")  # contains multidimensional columns, so not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45834009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def _class_spec_reader(fn):\n",
    "    \"\"\"Read the CLASS_SPEC from a WEAVE L2 FITS file as a Dataset.\n",
    "\n",
    "    All quantities are indexed by the `APS_ID` of the fibre.\n",
    "    Spectral quantities are additionally indexed by wavelength `LAMBDA_{B,R}`.\n",
    "    \"\"\"\n",
    "    cols = _read_fits_columns(fn, \"CLASS_SPEC\", limit_precision=True)\n",
    "    if not cols:\n",
    "        return None\n",
    "    coords = dict(APS_ID=cols.pop(\"APS_ID\"))\n",
    "    for c in list(cols):\n",
    "        if c.startswith(\"LAMBDA\"):\n",
    "            band = c.split(\"_\")[-1]\n",
    "            wl_all = cols.pop(c)\n",
    "            wl = wl_all[0]\n",
    "            assert (wl == wl_all).all()\n",
    "            coords[f\"LAMBDA_{band}\"] = wl\n",
    "    for c in cols:\n",
    "        dims = [\"APS_ID\"]\n",
    "        if c.endswith(\"_B\"):\n",
    "            dims += [\"LAMBDA_B\"]\n",
    "        elif c.endswith(\"_R\"):\n",
    "            dims += [\"LAMBDA_R\"]\n",
    "        cols[c] = xr.Variable(dims, cols[c], attrs={\"unit\": str(cols[c].unit)})\n",
    "    return xr.Dataset(cols, coords)\n",
    "\n",
    "\n",
    "read_class_spec = to_dataset(_class_spec_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e52c6-8043-4e7d-97d9-8e529fb24772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L526){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_class_spec\n",
       "\n",
       ">      read_class_spec (fn)\n",
       "\n",
       "Read the CLASS_SPEC from a WEAVE L2 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "Spectral quantities are additionally indexed by wavelength `LAMBDA_{B,R}`."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L526){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_class_spec\n",
       "\n",
       ">      read_class_spec (fn)\n",
       "\n",
       "Read the CLASS_SPEC from a WEAVE L2 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "Spectral quantities are additionally indexed by wavelength `LAMBDA_{B,R}`."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(read_class_spec, name=\"read_class_spec\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d974427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:07<00:00,  1.29s/it]\n",
      "Reading netCDF files... took 0.46 s. Size is 1703.853 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  50%|█████     | 3/6 [00:06<00:05,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  83%|████████▎ | 5/6 [00:06<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:07<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:07<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading netCDF files... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.45 s. Size is 1703.853 Mb\n"
     ]
    }
   ],
   "source": [
    "class_spec = read_class_spec(lr_l2_stack_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf450d-93c9-4c8a-8ad6-d9b824129c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 2GB\n",
      "Dimensions:     (APS_ID: 995, LAMBDA_B: 9648, LAMBDA_R: 15288, filename: 6)\n",
      "Coordinates:\n",
      "  * APS_ID      (APS_ID) int64 8kB 1 2 3 4 5 6 ... 1002 1003 1004 1005 1006 1007\n",
      "  * LAMBDA_B    (LAMBDA_B) float32 39kB 3.677e+03 3.677e+03 ... 6.089e+03\n",
      "  * LAMBDA_R    (LAMBDA_R) float32 61kB 5.774e+03 5.774e+03 ... 9.596e+03\n",
      "  * filename    (filename) <U32 768B 'stack_1003330__stack_1003329_APS' ... '...\n",
      "    OBID        (filename) <U4 96B dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "Data variables:\n",
      "    TARGID      (filename, APS_ID) object 48kB dask.array<chunksize=(1, 995), meta=np.ndarray>\n",
      "    CNAME       (filename, APS_ID) object 48kB dask.array<chunksize=(1, 995), meta=np.ndarray>\n",
      "    FLUX_RR_B   (filename, APS_ID, LAMBDA_B) float32 230MB dask.array<chunksize=(1, 995, 9648), meta=np.ndarray>\n",
      "    IVAR_RR_B   (filename, APS_ID, LAMBDA_B) float32 230MB dask.array<chunksize=(1, 995, 9648), meta=np.ndarray>\n",
      "    MODEL_RR_B  (filename, APS_ID, LAMBDA_B) float32 230MB dask.array<chunksize=(1, 995, 9648), meta=np.ndarray>\n",
      "    FLUX_RR_R   (filename, APS_ID, LAMBDA_R) float32 365MB dask.array<chunksize=(1, 995, 15288), meta=np.ndarray>\n",
      "    IVAR_RR_R   (filename, APS_ID, LAMBDA_R) float32 365MB dask.array<chunksize=(1, 995, 15288), meta=np.ndarray>\n",
      "    MODEL_RR_R  (filename, APS_ID, LAMBDA_R) float32 365MB dask.array<chunksize=(1, 995, 15288), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "print(class_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15202991-dc49-45b2-9884-37805f005aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_spec.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6caf780-7b1d-4c63-87a9-e5e0a317916e",
   "metadata": {},
   "source": [
    "### STAR_SPEC\n",
    "\n",
    "The STAR_SPEC extension contains spectra and model fits for stellar measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852a937-2588-45c6-b7c8-02bfa8d09e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table.read(l2_hdus[\"STAR_SPEC\"], unit_parse_strict=\"silent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f149f81-ee33-4428-9ce3-5b0618f48a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def _star_spec_reader(fn):\n",
    "    \"\"\"Read the STAR_SPEC from a WEAVE L2 FITS file as a Dataset.\n",
    "\n",
    "    All quantities are indexed by the `APS_ID` of the fibre.\n",
    "    Spectral quantities are additionally indexed by wavelength bin `LAMBIN_{R,B,C}`,\n",
    "    which does *not* correspond to the same wavelength for each spectrum.\n",
    "    \"\"\"\n",
    "    cols = _read_fits_columns(fn, \"STAR_SPEC\", limit_precision=True)\n",
    "    if not cols:\n",
    "        return None\n",
    "    coords = dict(APS_ID=cols.pop(\"APS_ID\"))\n",
    "    for c in cols:\n",
    "        dims = [\"APS_ID\"]\n",
    "        if c.endswith(\"_B\"):\n",
    "            dims += [\"LAMBIN_B\"]\n",
    "        elif c.endswith(\"_R\"):\n",
    "            dims += [\"LAMBIN_R\"]\n",
    "        elif c.endswith(\"_C\"):\n",
    "            dims += [\"LAMBIN_C\"]\n",
    "        cols[c] = xr.Variable(dims, cols[c], attrs={\"unit\": str(cols[c].unit)})\n",
    "    return xr.Dataset(cols, coords)\n",
    "\n",
    "\n",
    "read_star_spec = to_dataset(_star_spec_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df7e6fb-a1fd-43c9-8ff8-1e5542ae8892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L556){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_star_spec\n",
       "\n",
       ">      read_star_spec (fn)\n",
       "\n",
       "Read the STAR_SPEC from a WEAVE L2 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "Spectral quantities are additionally indexed by wavelength bin `LAMBIN_{R,B,C}`,\n",
       "which does *not* correspond to the same wavelength for each spectrum."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L556){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_star_spec\n",
       "\n",
       ">      read_star_spec (fn)\n",
       "\n",
       "Read the STAR_SPEC from a WEAVE L2 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "Spectral quantities are additionally indexed by wavelength bin `LAMBIN_{R,B,C}`,\n",
       "which does *not* correspond to the same wavelength for each spectrum."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(read_star_spec, name=\"read_star_spec\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6cf690-5bd6-4529-b45b-98d0b512308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:00<00:00,  7.85it/s]\n",
      "Reading netCDF files... took 0.92 s. Size is 106.808 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  83%|████████▎ | 5/6 [00:00<00:00, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:00<00:00,  9.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading netCDF files... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.73 s. Size is 106.808 Mb\n"
     ]
    }
   ],
   "source": [
    "star_spec = read_star_spec(lr_l2_stack_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06397d39-4702-421c-be3e-4d3273fb00a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 112MB\n",
      "Dimensions:       (APS_ID: 24, filename: 6, LAMBIN_B: 9648, LAMBIN_R: 15288,\n",
      "                   LAMBIN_C: 23672)\n",
      "Coordinates:\n",
      "  * APS_ID        (APS_ID) int64 192B 194 222 283 304 329 ... 922 946 948 992\n",
      "  * filename      (filename) <U32 768B 'stack_1003426__stack_1003425_APS' ......\n",
      "    OBID          (filename) <U4 96B dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "Dimensions without coordinates: LAMBIN_B, LAMBIN_R, LAMBIN_C\n",
      "Data variables: (12/14)\n",
      "    TARGID        (filename, APS_ID) object 1kB dask.array<chunksize=(1, 24), meta=np.ndarray>\n",
      "    CNAME         (filename, APS_ID) object 1kB dask.array<chunksize=(1, 24), meta=np.ndarray>\n",
      "    LAMBDA_RVS_B  (filename, APS_ID, LAMBIN_B) float32 6MB dask.array<chunksize=(1, 24, 9648), meta=np.ndarray>\n",
      "    FLUX_RVS_B    (filename, APS_ID, LAMBIN_B) float32 6MB dask.array<chunksize=(1, 24, 9648), meta=np.ndarray>\n",
      "    ERROR_RVS_B   (filename, APS_ID, LAMBIN_B) float32 6MB dask.array<chunksize=(1, 24, 9648), meta=np.ndarray>\n",
      "    MODEL_RVS_B   (filename, APS_ID, LAMBIN_B) float32 6MB dask.array<chunksize=(1, 24, 9648), meta=np.ndarray>\n",
      "    ...            ...\n",
      "    ERROR_RVS_R   (filename, APS_ID, LAMBIN_R) float32 9MB dask.array<chunksize=(1, 24, 15288), meta=np.ndarray>\n",
      "    MODEL_RVS_R   (filename, APS_ID, LAMBIN_R) float32 9MB dask.array<chunksize=(1, 24, 15288), meta=np.ndarray>\n",
      "    LAMBDA_FR_C   (filename, APS_ID, LAMBIN_C) float32 14MB dask.array<chunksize=(1, 24, 23672), meta=np.ndarray>\n",
      "    FLUX_FR_C     (filename, APS_ID, LAMBIN_C) float32 14MB dask.array<chunksize=(1, 24, 23672), meta=np.ndarray>\n",
      "    ERROR_FR_C    (filename, APS_ID, LAMBIN_C) float32 14MB dask.array<chunksize=(1, 24, 23672), meta=np.ndarray>\n",
      "    MODEL_FR_C    (filename, APS_ID, LAMBIN_C) float32 14MB dask.array<chunksize=(1, 24, 23672), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "print(star_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4bd67-c61d-40d2-887e-b37ccac35443",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_spec.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b184bb0d-fec9-42f3-a0ce-115aef36df3e",
   "metadata": {},
   "source": [
    "### GALAXY_SPEC\n",
    "\n",
    "The GALAXY_SPEC extension contains log-wavelength-binned spectra and model fits by PPXF and GANDALF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7df2d-01b3-406f-83f1-8bb757a3dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table.read(l2_hdus[\"GALAXY_SPEC\"], unit_parse_strict=\"silent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d87de-d743-443a-aee1-0125850a3112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def _galaxy_spec_reader(fn):\n",
    "    \"\"\"Read the GALAXY_SPEC from a WEAVE L2 FITS file as a Dataset.\n",
    "\n",
    "    All quantities are indexed by the `APS_ID` of the fibre.\n",
    "    Spectral quantities are additionally indexed by log-wavelength bin `LOGLAMBIN`,\n",
    "    which does *not* correspond to the same wavelength for each spectrum.\n",
    "    \"\"\"\n",
    "    cols = _read_fits_columns(fn, \"GALAXY_SPEC\", limit_precision=True)\n",
    "    if not cols:\n",
    "        return None\n",
    "    coords = dict(APS_ID=cols.pop(\"APS_ID\"))\n",
    "    for c in cols:\n",
    "        dims = [\"APS_ID\"]\n",
    "        if c.endswith(\"_PPXF\") or c.endswith(\"_GAND\"):\n",
    "            dims += [\"LOGLAMBIN\"]\n",
    "        cols[c] = xr.Variable(dims, cols[c], attrs={\"unit\": str(cols[c].unit)})\n",
    "    return xr.Dataset(cols, coords)\n",
    "\n",
    "\n",
    "read_galaxy_spec = to_dataset(_galaxy_spec_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c375a5-505f-4210-afe3-04c5c361de16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L582){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_galaxy_spec\n",
       "\n",
       ">      read_galaxy_spec (fn)\n",
       "\n",
       "Read the GALAXY_SPEC from a WEAVE L2 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "Spectral quantities are additionally indexed by log-wavelength bin `LOGLAMBIN`,\n",
       "which does *not* correspond to the same wavelength for each spectrum."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/data.py#L582){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### read_galaxy_spec\n",
       "\n",
       ">      read_galaxy_spec (fn)\n",
       "\n",
       "Read the GALAXY_SPEC from a WEAVE L2 FITS file as a Dataset.\n",
       "\n",
       "All quantities are indexed by the `APS_ID` of the fibre.\n",
       "Spectral quantities are additionally indexed by log-wavelength bin `LOGLAMBIN`,\n",
       "which does *not* correspond to the same wavelength for each spectrum."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(read_galaxy_spec, name=\"read_galaxy_spec\", title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471c3ab-75e2-4e6c-973b-b4d4f311a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:21<00:00,  3.62s/it]\n",
      "Reading netCDF files... took 0.85 s. Size is 10232.270 Mb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  50%|█████     | 3/6 [00:24<00:18,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  67%|██████▋   | 4/6 [00:26<00:09,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary:  83%|████████▎ | 5/6 [00:28<00:03,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:29<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Locating and converting where necessary: 100%|██████████| 6/6 [00:29<00:00,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading netCDF files... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.74 s. Size is 10232.270 Mb\n"
     ]
    }
   ],
   "source": [
    "galaxy_spec = read_galaxy_spec(lr_l2_stack_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a337f9b-ba95-49c6-8202-671c2011fb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 11GB\n",
      "Dimensions:           (APS_ID: 994, filename: 6, LOGLAMBIN: 23671)\n",
      "Coordinates:\n",
      "  * APS_ID            (APS_ID) int32 4kB 1 2 3 4 5 ... 1003 1004 1005 1006 1007\n",
      "  * filename          (filename) <U32 768B 'stack_1003330__stack_1003329_APS'...\n",
      "    OBID              (filename) <U4 96B dask.array<chunksize=(1,), meta=np.ndarray>\n",
      "Dimensions without coordinates: LOGLAMBIN\n",
      "Data variables: (12/15)\n",
      "    TARGID            (filename, APS_ID) object 48kB dask.array<chunksize=(1, 994), meta=np.ndarray>\n",
      "    CNAME             (filename, APS_ID) object 48kB dask.array<chunksize=(1, 994), meta=np.ndarray>\n",
      "    LOGLAM_PPXF       (filename, APS_ID, LOGLAMBIN) float32 565MB dask.array<chunksize=(1, 994, 23671), meta=np.ndarray>\n",
      "    FLUX_PPXF         (filename, APS_ID, LOGLAMBIN) float32 565MB dask.array<chunksize=(1, 994, 23671), meta=np.ndarray>\n",
      "    ERROR_PPXF        (filename, APS_ID, LOGLAMBIN) float32 565MB dask.array<chunksize=(1, 994, 23671), meta=np.ndarray>\n",
      "    MODEL_PPXF        (filename, APS_ID, LOGLAMBIN) float32 565MB dask.array<chunksize=(1, 994, 23671), meta=np.ndarray>\n",
      "    ...                ...\n",
      "    ERROR_GAND        (filename, APS_ID, LOGLAMBIN) float32 565MB dask.array<chunksize=(1, 994, 23671), meta=np.ndarray>\n",
      "    MODEL_GAND        (filename, APS_ID, LOGLAMBIN) float64 1GB dask.array<chunksize=(1, 994, 23671), meta=np.ndarray>\n",
      "    EMISSION_GAND     (filename, APS_ID, LOGLAMBIN) float64 1GB dask.array<chunksize=(1, 994, 23671), meta=np.ndarray>\n",
      "    FLUX_CLEAN_GAND   (filename, APS_ID, LOGLAMBIN) float64 1GB dask.array<chunksize=(1, 994, 23671), meta=np.ndarray>\n",
      "    MODEL_CLEAN_GAND  (filename, APS_ID, LOGLAMBIN) float64 1GB dask.array<chunksize=(1, 994, 23671), meta=np.ndarray>\n",
      "    GOODPIX_GAND      (filename, APS_ID, LOGLAMBIN) float64 1GB dask.array<chunksize=(1, 994, 23671), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "print(galaxy_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1342299-1185-4c90-b090-7b9cc27c7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy_spec.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fe22455-ea94-406e-a85f-8f9a614abae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
