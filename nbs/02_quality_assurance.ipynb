{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0493e6",
   "metadata": {},
   "source": [
    "# quality_assurance\n",
    "\n",
    "> In progress, including development of diagnostics to be moved elsewhere when complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23332175-f8be-4b76-927d-438042c90a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp quality_assurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e567803-7cc7-4f75-bfd0-27bc4b81bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import dask\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727cd5c3-93ed-47b2-bc6f-5407ace40e3a",
   "metadata": {},
   "source": [
    "To write checks of the data, we first create a subclass of `Diagnostics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484e6ad-0dd9-4879-9033-ce9d01939398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "class Diagnostics(ABC):\n",
    "    \"\"\"An abstract class to be subclassed to perform specific diagnostic checks.\n",
    "\n",
    "    A subclass should perform a set of checks, implemented in a method named `tests`.\n",
    "\n",
    "    Calling the method `run` will combine and compute the tests, putting the results\n",
    "    as a single boolean `DataArray` the `detail` attribute for further analysis.\n",
    "\n",
    "    Once `run` has been called, a `test_descriptions` will also be available, and\n",
    "    summaries of the test results can be created using the `summary` method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialise the diagnostic.\n",
    "\n",
    "        If the subclass has options, these may be added like this:\n",
    "        ```\n",
    "        def __init__(\n",
    "            self,\n",
    "            tolerance: float = 0.0,  # the tolerance for the test\n",
    "        ):\n",
    "            self.tolerance = tolerance\n",
    "            super().__init__()\n",
    "        ```\n",
    "        then `self.tolerance` can be used in the implementation of `tests`.\n",
    "        \"\"\"\n",
    "        self.test_descriptions = None\n",
    "        self.detail = None\n",
    "\n",
    "    def run(self, **kwargs) -> xr.DataArray:\n",
    "        \"\"\"Compute the results of the tests.\n",
    "\n",
    "        The `kwargs` are passed to `qagmire.data.read_*` functions to obtain the data\n",
    "        for the tests.\n",
    "        \"\"\"\n",
    "        tests = self.tests(**kwargs)\n",
    "        test_names = [t[\"name\"] for t in tests]\n",
    "        test_desc = [t[\"description\"] for t in tests]\n",
    "        self.test_descriptions = dict(zip(test_names, test_desc))\n",
    "        test_array = [t[\"test\"] for t in tests]\n",
    "        detail = xr.concat(test_array, pd.Index(test_names, name=\"test\"))\n",
    "        start = time.perf_counter()\n",
    "        self.detail = dask.compute(detail)[0]\n",
    "        dt = time.perf_counter() - start\n",
    "        print(f\"Tests took {dt:.2f} s to perform.\")\n",
    "        for name, desc in self.test_descriptions.items():\n",
    "            print(f\"{name}:\\n    {desc}\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def tests(self, **kwargs):\n",
    "        \"\"\"Return the tests to be performed.\n",
    "\n",
    "        Implementations of this method must pass `kwargs` to `qagmire.data.read_*` functions\n",
    "        as necessary to obtain the data for the tests.\n",
    "\n",
    "        This method must returns a list of dictionaries with the structure:\n",
    "        ```\n",
    "        [\n",
    "            {\n",
    "                \"name\": \"a_short_name\",\n",
    "                \"description\": \"The question that the test answers\",\n",
    "                \"test\": test_dataset,\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "        ```\n",
    "        where each `test_dataset` should be a boolean `xr.DataArray` of the same shape, giving\n",
    "        the results of running the test on the data defined by `kwargs`.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"name\": \"a_short_name\",\n",
    "                \"description\": \"The question that the test answers\",\n",
    "                \"test\": None,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    def summary(\n",
    "        self,\n",
    "        by: str | None = None,  # optionally sum element dims except for this one\n",
    "        show_passed_tests=False,  # if `True`, then passed tests are included\n",
    "        show_passed_elements=False,  # if `True`, then passed elements are included\n",
    "        sort_by_total_fails=True,  # if `False`, then keep in original order\n",
    "        show_failure_count=True,  # if `False`, then omit the count of failures per row\n",
    "        show_only_failure_count=False,  # if `True`, then only show the count of failures\n",
    "        per_test=False,  # if `True`, then transpose output, such that each row is a test\n",
    "        top: int | None = None,  # optionally limit to at most `top` elements\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Return a summary of the test failures.\"\"\"\n",
    "        if self.detail is None:\n",
    "            print(\"You need to call `run` first.\")\n",
    "            return None\n",
    "        else:\n",
    "            detail = self.detail\n",
    "        if by is not None:\n",
    "            detail = detail.sum(dim=[d for d in detail.dims if d not in (\"test\", by)])\n",
    "        # convert to a dataframe and drop any extra coordinates\n",
    "        df = detail.to_dataframe(name=\"failed\")[[\"failed\"]]\n",
    "        df = df.unstack() if per_test else df.unstack(\"test\")\n",
    "        if (not show_passed_tests and not per_test) or (\n",
    "            not show_passed_elements and per_test\n",
    "        ):\n",
    "            df = df.loc[:, df.any(axis=\"rows\")]\n",
    "        if (not show_passed_elements and not per_test) or (\n",
    "            not show_passed_tests and per_test\n",
    "        ):\n",
    "            df = df.loc[df.loc[:, \"failed\"].any(axis=\"columns\")]\n",
    "        df.loc[:, \"total fails\"] = df.sum(axis=\"columns\")\n",
    "        if sort_by_total_fails:\n",
    "            df = df.sort_values(\"total fails\", ascending=False)\n",
    "        if not (show_failure_count or show_only_failure_count):\n",
    "            df = df.drop(columns=\"total fails\")\n",
    "        if show_only_failure_count:\n",
    "            df = df.drop(columns=\"failed\")\n",
    "        if top is not None:\n",
    "            df = df.iloc[:top]\n",
    "        return df\n",
    "\n",
    "    def summary_per_test(\n",
    "        self,\n",
    "        by: str | None = None,  # optionally sum element dims except for this one\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Return a per-test summary of the test outcomes in `detail`.\"\"\"\n",
    "        return self.summary(\n",
    "            by=by,\n",
    "            per_test=True,\n",
    "            show_passed_tests=True,\n",
    "            show_only_failure_count=True,\n",
    "        )\n",
    "\n",
    "    def full_summary(\n",
    "        self,\n",
    "        by: str | None = None,  # optionally sum element dims except for this one\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Return a full summary of the test outcomes in `detail`.\"\"\"\n",
    "        return self.summary(\n",
    "            by=by,\n",
    "            show_passed_tests=True,\n",
    "            show_passed_elements=True,\n",
    "            sort_by_total_fails=False,\n",
    "            show_failure_count=False,\n",
    "            top=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdbe2b5-7369-434c-8f78-6c41a27e1d43",
   "metadata": {},
   "source": [
    "In this subclass we need to implement the `tests` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495e2b4-69f0-4926-895b-0b0e61bb3fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/quality_assurance.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Diagnostics.tests\n",
       "\n",
       ">      Diagnostics.tests (**kwargs)\n",
       "\n",
       "Return the tests to be performed.\n",
       "\n",
       "Implementations of this method must pass `kwargs` to `qagmire.data.read_*` functions\n",
       "as necessary to obtain the data for the tests.\n",
       "\n",
       "This method must returns a list of dictionaries with the structure:\n",
       "```\n",
       "[\n",
       "    {\n",
       "        \"name\": \"a_short_name\",\n",
       "        \"description\": \"The question that the test answers\",\n",
       "        \"test\": test_dataset,\n",
       "    },\n",
       "    ...\n",
       "]\n",
       "```\n",
       "where each `test_dataset` should be a boolean `xr.DataArray` of the same shape, giving\n",
       "the results of running the test on the data defined by `kwargs`."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/quality_assurance.py#L75){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Diagnostics.tests\n",
       "\n",
       ">      Diagnostics.tests (**kwargs)\n",
       "\n",
       "Return the tests to be performed.\n",
       "\n",
       "Implementations of this method must pass `kwargs` to `qagmire.data.read_*` functions\n",
       "as necessary to obtain the data for the tests.\n",
       "\n",
       "This method must returns a list of dictionaries with the structure:\n",
       "```\n",
       "[\n",
       "    {\n",
       "        \"name\": \"a_short_name\",\n",
       "        \"description\": \"The question that the test answers\",\n",
       "        \"test\": test_dataset,\n",
       "    },\n",
       "    ...\n",
       "]\n",
       "```\n",
       "where each `test_dataset` should be a boolean `xr.DataArray` of the same shape, giving\n",
       "the results of running the test on the data defined by `kwargs`."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(Diagnostics.tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f27edb-d745-4ce7-aa4c-476ce66b1862",
   "metadata": {},
   "source": [
    "These tests are executed by calling the `run` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c15018-94bc-45ca-8dc5-c337c6739b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/quality_assurance.py#L55){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Diagnostics.run\n",
       "\n",
       ">      Diagnostics.run (**kwargs)\n",
       "\n",
       "Compute the results of the tests.\n",
       "\n",
       "The `kwargs` are passed to `qagmire.data.read_*` functions to obtain the data\n",
       "for the tests."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/quality_assurance.py#L55){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Diagnostics.run\n",
       "\n",
       ">      Diagnostics.run (**kwargs)\n",
       "\n",
       "Compute the results of the tests.\n",
       "\n",
       "The `kwargs` are passed to `qagmire.data.read_*` functions to obtain the data\n",
       "for the tests."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(Diagnostics.run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874c2cb3-caf1-4b6d-a128-7af8396764c5",
   "metadata": {},
   "source": [
    "The `summary` method outputs a pandas DataFrame summary of the test outcomes, by default this shows only failed tests and elements (e.g. OBs or exposures) with the most failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb0f45b-04a7-4fca-ad2a-0e39273e0631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/quality_assurance.py#L103){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Diagnostics.summary\n",
       "\n",
       ">      Diagnostics.summary (by:str|None=None, show_passed_tests=False,\n",
       ">                           show_passed_elements=False,\n",
       ">                           sort_by_total_fails=True, show_failure_count=True,\n",
       ">                           show_only_failure_count=False, per_test=False,\n",
       ">                           top:int|None=None)\n",
       "\n",
       "Return a summary of the test failures.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| by | str \\| None | None | optionally sum element dims except for this one |\n",
       "| show_passed_tests | bool | False | if `True`, then passed tests are included |\n",
       "| show_passed_elements | bool | False | if `True`, then passed elements are included |\n",
       "| sort_by_total_fails | bool | True | if `False`, then keep in original order |\n",
       "| show_failure_count | bool | True | if `False`, then omit the count of failures per row |\n",
       "| show_only_failure_count | bool | False | if `True`, then only show the count of failures |\n",
       "| per_test | bool | False | if `True`, then transpose output, such that each row is a test |\n",
       "| top | int \\| None | None | optionally limit to at most `top` elements |\n",
       "| **Returns** | **DataFrame** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/bamford/qagmire/blob/main/qagmire/quality_assurance.py#L103){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Diagnostics.summary\n",
       "\n",
       ">      Diagnostics.summary (by:str|None=None, show_passed_tests=False,\n",
       ">                           show_passed_elements=False,\n",
       ">                           sort_by_total_fails=True, show_failure_count=True,\n",
       ">                           show_only_failure_count=False, per_test=False,\n",
       ">                           top:int|None=None)\n",
       "\n",
       "Return a summary of the test failures.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| by | str \\| None | None | optionally sum element dims except for this one |\n",
       "| show_passed_tests | bool | False | if `True`, then passed tests are included |\n",
       "| show_passed_elements | bool | False | if `True`, then passed elements are included |\n",
       "| sort_by_total_fails | bool | True | if `False`, then keep in original order |\n",
       "| show_failure_count | bool | True | if `False`, then omit the count of failures per row |\n",
       "| show_only_failure_count | bool | False | if `True`, then only show the count of failures |\n",
       "| per_test | bool | False | if `True`, then transpose output, such that each row is a test |\n",
       "| top | int \\| None | None | optionally limit to at most `top` elements |\n",
       "| **Returns** | **DataFrame** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |hide\n",
    "show_doc(Diagnostics.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d0dd3c-3ddc-4dc0-8623-a61f43a56cec",
   "metadata": {},
   "source": [
    "See the [diagnostics](diagnostics) submodule for example tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca8bb45-e1aa-4315-9aa1-14e36190cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
