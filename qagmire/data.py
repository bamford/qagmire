# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_data.ipynb.

# %% auto 0
__all__ = ['to_dataset', 'to_dataset_without_cache', 'read_primary_header', 'read_raw_data', 'read_fibre_table',
           'read_fibre_table_nspec', 'read_l1_data', 'read_class_table', 'read_star_table', 'read_galaxy_table',
           'read_class_spec', 'read_star_spec', 'read_galaxy_spec', 'FITStoDataset', 'get_weave_files',
           'get_lr_raw_files', 'get_lr_l1_single_files', 'get_lr_l2_stack_files']

# %% ../nbs/01_data.ipynb 3
import os
import sys
import time
from functools import partial, wraps
from glob import glob
from multiprocessing import Pool
from typing import Callable

import numpy as np
import xarray as xr
from astropy.io import fits
from astropy.table import Table
from tqdm import tqdm

from .utilities import mjd_to_night

# %% ../nbs/01_data.ipynb 6
def _read_single(
    read_function: Callable[[str], xr.Dataset],  # function to read a FITS file
    fn: str,  # filename of FITS file to read
):
    """Read a FITS file to an xarray Dataset using the given read function."""
    ds = read_function(fn)
    if ds:
        fn_base = os.path.splitext(os.path.basename(fn))[0]
        ds = ds.expand_dims({"filename": [fn_base]})
        try:
            run = fits.getval(fn, "RUN")
            ds = ds.assign_coords(RUN=("filename", [run]))
        except KeyError:
            pass
        try:
            camera = fits.getval(fn, "CAMERA")
            camera = camera.replace("WEAVE", "")
            ds = ds.assign_coords(CAMERA=("filename", [camera]))
        except KeyError:
            pass
        try:
            mjd = np.round(fits.getval(fn, "MJD-OBS"), 4)
            ds = ds.assign_coords(MJD=("filename", [mjd]))
            night = mjd_to_night(mjd)
            ds = ds.assign_coords(NIGHT=("filename", [night]))
        except KeyError:
            pass
        try:
            obid = fits.getval(fn, "OBID")
            ds = ds.assign_coords(OBID=("filename", [obid]))
        except KeyError:
            pass
    else:
        table = read_function.__name__.replace("read_", "")
        print(f"Warning: cannot read {table} for file {fn}.")
    return ds


def _single_via_netcdf(
    read_function: Callable[[str], xr.Dataset],  # function to read a FITS file
    fn: str,  # filename of FITS file to read
    netcdf_store,
    update_cache,
):
    """Transform a FITS file to netCDF using the given read function.

    Returns the path of a netCDF file stored in `nedcdf_store`, containing the `Dataset` resulting
    from calling `read_function` with the supplied FITS filename `fn`. If the netCDF file already
    exists, the filename is immediately returned.
    """
    fn_netcdf = os.path.join(netcdf_store, *os.path.normpath(fn).split(os.sep)[-3:])
    table = read_function.__name__.replace("read_", "")
    fn_netcdf = os.path.splitext(fn_netcdf)[0]
    fn_netcdf = f"{fn_netcdf}_{table}.nc"
    if not os.path.exists(fn_netcdf) or update_cache:
        ds = _read_single(read_function, fn)
        if ds:
            os.makedirs(os.path.dirname(fn_netcdf), exist_ok=True)
            ds.to_netcdf(fn_netcdf, format="NETCDF4", engine="netcdf4")
            ds.close()
        else:
            fn_netcdf = None
    return fn_netcdf


class FITStoDataset:
    """Access multiple FITS tables as an xarray Dataset, optionally via cached netCDF files.

    For each FITS table or image we wish to read, we will write a `read_*(fn)` function
    which reads the table from the single provided filename `fn` and returns a `Dataset`.
    Wrapping such a function with an instance of this class adapts the function to take
    a list of FITS filenames and return a `Dataset`. If `cache=True`, the Dataset is
    lazily loaded data from a cache of netCDF files. The cache is stored in the
    `netcdf_store` folder defined when the instance is initialised.

    If `cache=True`, when the wrapped function is initially run, it repeatedly calls
    `single_via_netcdf` to apply the original `read_*` function to each FITS filename and
    save each resulting `Dataset` as a netCDF file, then opens them together and returns a
    combined, distributed `Dataset`. If a previously converted file is found in the
    `netcdf_store`, then the original `read_*` function is skipped and the netCDF file loaded
    directly. This caching can vastly increase the speed of subsequent calls.
    If `n_processes > 1`, which it is by default, then this reading and caching is performed
    in parallel using `n_processes` processes.

    Although instances of this class can be used as a decorator, doing so with
    `n_processes > 1` will lead to an exception due to pickling issues with multiprocessing.
    Instead they should be used to wrap functions, without replacing the original function name.
    For example,
    ```
    to_dataset = FITStoDataset()

    def _class_spec_reader(fn):
        ...

    read_class_spec = to_dataset(_class_spec_reader)
    ```

    If a source FITS file is changed, the corresponding files in `netcdf_store` can simply
    be deleted and they will be recreated on the next call of the decorated `read_*`
    function.

    If `cache=False`, then the data is always read from the specified FITS files, combined
    and returned as an in-memory Dataset. This may be faster when dealing with lots of small
    files.

    If `update_cache=False`, then existing cache files are not read, but are recreated.

    """

    def __init__(
        self,
        cache=True,  # cache the dataset to netCDF files
        netcdf_store: str | None = None,  # folder in which to store the netCDF files
        progress=True,  # display a progress bar
        update_cache=False,  # read FITS files and recreate netCDF files, no effect if cache=False
        n_processes=8,  # how many subprocesses to use
    ):
        """Create a decorator that can extend a `read_*` function to multiple files.

        If no `netcdf_store` is provided it first checks for a `NETCDF_STORE` environment
        variable and falls back to a folder called `netcdf_store` in the user's home folder.
        """
        self.cache = cache
        self.update_cache = update_cache
        self.n_processes = n_processes

        if netcdf_store is not None:
            self.netcdf_store = netcdf_store
        else:
            default = "/beegfs/weavelofar/netcdf_store"
            self.netcdf_store = os.environ.get("NETCDF_STORE", default)

        if progress:
            if cache:
                desc = "Locating and converting where necessary"
            else:
                desc = "Reading files"
            self.progress = partial(tqdm, desc=desc, file=sys.stdout)
        else:
            self.progress = lambda x: x

    def __call__(
        self,
        read_function: Callable[[str], xr.Dataset],  # function to read a FITS file
    ):
        """Extend the functionality of `read_function` to multiple files.

        The wrapped `read_function` is adapted to take a list of FITS filenames and
        return a `Dataset`, which lazily loads data from a cache of netCDF files if
        `self.cache=True` (the default).
        """

        @wraps(read_function)
        def wrapper(fns):
            if self.cache:
                read = partial(
                    _single_via_netcdf,
                    read_function,
                    netcdf_store=self.netcdf_store,
                    update_cache=self.update_cache,
                )
            else:
                read = partial(_read_single, read_function)

            if self.n_processes > 1:
                results = []
                try:
                    with Pool(self.n_processes) as p:
                        with self.progress(total=len(fns)) as pbar:
                            for f in p.imap_unordered(read, fns):
                                results.append(f)
                                pbar.update()
                except PermissionError:
                    raise PermissionError(
                        "Cannot access the NetCDF file. Ensure any previously "
                        "created Datasets are closed, e.g. ds.close()"
                    )
            else:
                results = [read(fn) for fn in self.progress(fns)]

            results = [f for f in results if f is not None]

            if self.cache:
                print("Reading netCDF files... ", end="")
                start = time.perf_counter()
                data = xr.open_mfdataset(
                    results,
                    parallel=True,
                    combine="nested",
                    coords="minimal",
                    concat_dim="filename",
                    engine="netcdf4",
                )
            else:
                print("Creating Dataset... ", end="")
                start = time.perf_counter()
                data = xr.concat(results, dim="filename", coords="minimal")
            dt = time.perf_counter() - start
            print(f"took {dt:.2f} s. Size is {data.nbytes * 2**-20:.3f} Mb")
            return data

        return wrapper

# %% ../nbs/01_data.ipynb 8
to_dataset = FITStoDataset()
to_dataset_without_cache = FITStoDataset(cache=False)

# %% ../nbs/01_data.ipynb 12
def _is_lowres(fn):
    """Check the header of FITS file `fn` to determine if it is low-resolution."""
    try:
        lowres = "LR" in fits.getval(fn, "RES-OBS")
    except KeyError:
        lowres = "LOWRES" in fits.getval(fn, "MODE")
    return lowres


def get_weave_files(
    level="*",  # pattern to match to the file level, e.g. raw, L1, L2
    filetype="*",  # pattern to match to the file type, e.g. single, stack
    date="*",  # pattern to match to the date in format yyyymmdd
    runid="*",  # pattern to match to the runid
    lowres=True,  # select low-res files, or high-res if False
):
    """Get a list of matching WEAVE files."""
    if level != "raw":
        filetype += "_"
    pattern = f"{level}/{date}/{filetype}*{runid}*.fit*"
    pattern = os.path.join(os.environ["WEAVEIO_ROOTDIR"], pattern)
    files = glob(pattern)
    files.sort()
    if lowres:
        files = [fn for fn in files if _is_lowres(fn)]
    else:
        files = [fn for fn in files if not _is_lowres(fn)]
    return files


def get_lr_raw_files(
    date="*",  # pattern to match to the date in format yyyymmdd
    runid="*",  # pattern to match to the runid
):
    return get_weave_files(level="raw", date=date, runid=runid, lowres=True)


def get_lr_l1_single_files(
    date="*",  # pattern to match to the date in format yyyymmdd
    runid="*",  # pattern to match to the runid
):
    return get_weave_files(
        level="L1", filetype="single", date=date, runid=runid, lowres=True
    )


def get_lr_l2_stack_files(
    date="*",  # pattern to match to the date in format yyyymmdd
    runid="*",  # pattern to match to the runid
):
    return get_weave_files(
        level="L2", filetype="stack", date=date, runid=runid, lowres=True
    )

# %% ../nbs/01_data.ipynb 13
def _read_fits_columns(
    fn: str,  # the filename of the FITS file to read
    ext: str,  # the name of the extension containing the table to read
    limit_precision=False,  # convert all float64 columns to float32
    index: str | None = None,  # remove rows where this column is masked
):
    """Read a FITS table to a dict of arrays and convert endianness."""
    cols = dict(Table.read(fn, ext, unit_parse_strict="silent"))
    cols = {c: cols[c].newbyteorder().byteswap() for c in cols}
    if limit_precision:
        for c in list(cols):
            if cols[c].dtype.type is np.float64:
                if np.can_cast(np.max(np.abs(cols[c])), np.float32):
                    cols[c] = cols[c].astype(np.float32)
    if index is not None:
        ok = ~cols[index].mask
        cols = {c: cols[c][ok] for c in cols}
    return cols

# %% ../nbs/01_data.ipynb 22
def _primary_header_reader(fn):
    """Read the primary header as a Dataset, stripping comments."""
    hdr = fits.getheader(fn, "PRIMARY")
    for key in hdr:
        if key == "" or "COMM" in key:
            del hdr[key]
    return xr.Dataset(hdr)


read_primary_header = to_dataset_without_cache(_primary_header_reader)

# %% ../nbs/01_data.ipynb 27
def _raw_data_reader(fn):
    """Read the *_DATA from a WEAVE RAW FITS file as a Dataset."""
    hdus = fits.open(fn)
    for h in hdus:
        if h.name.endswith("1_DATA"):
            counts1 = xr.DataArray(h.data)
        elif h.name.endswith("2_DATA"):
            counts2 = xr.DataArray(h.data)
    return xr.Dataset({"counts1": counts1, "counts2": counts2})


read_raw_data = to_dataset(_raw_data_reader)

# %% ../nbs/01_data.ipynb 35
def _fibre_table_reader_indexed(fn, index_by_nspec=True):
    cols = _read_fits_columns(fn, "FIBTABLE", index="FIBREID")
    cols = {c.upper(): cols[c] for c in cols}
    if not cols:
        return None
    coords = dict(APS_ID=cols.pop("FIBREID"))
    for c in cols:
        dims = ["APS_ID"]
        cols[c] = xr.Variable(dims, cols[c], attrs={"unit": str(cols[c].unit)})
    ds = xr.Dataset(cols, coords)
    if index_by_nspec:
        ds = ds.set_coords("NSPEC").swap_dims(APS_ID="NSPEC").reset_coords("APS_ID")
    return ds


def _fibre_table_reader(fn):
    """Read the FIBTABLE from a WEAVE RAW or L1 FITS file as a Dataset.

    All quantities are indexed by the `APS_ID` of the fibre.

    All column names are mde uppercase for consistency.
    """
    return _fibre_table_reader_indexed(fn, index_by_nspec=False)


def _fibre_table_reader_nspec(fn):
    """Read the FIBTABLE from a WEAVE L1 FITS file as a Dataset.

    All quantities are indexed by the `NSPEC` of the fibre.

    All column names are mde uppercase for consistency.
    """
    return _fibre_table_reader_indexed(fn, index_by_nspec=True)


read_fibre_table = to_dataset_without_cache(_fibre_table_reader)
read_fibre_table_nspec = to_dataset_without_cache(_fibre_table_reader_nspec)

# %% ../nbs/01_data.ipynb 55
def _l1_data_reader(fn):
    """Read the data from a WEAVE L1 FITS file as a Dataset."""
    hdus = fits.open(fn)
    camera = hdus["PRIMARY"].header["CAMERA"].replace("WEAVE", "")
    band = camera[0]
    hdr = hdus[f"{camera}_DATA"].header
    increment, zeropoint, size_wl, size_nspec = (
        hdr["CD1_1"],
        hdr["CRVAL1"],
        hdr["NAXIS1"],
        hdr["NAXIS2"],
    )
    wl = np.arange(0, size_wl) * increment + zeropoint
    nspec = np.arange(1, size_nspec + 1)
    coords = {"NSPEC": nspec, f"LAMBDA_{band}": wl}
    dims = list(coords.keys())
    arrays = {}
    for ext in ["DATA", "IVAR", "DATA_NOSS", "IVAR_NOSS", "SENSFUNC"]:
        name = f"{camera}_{ext}"
        data = hdus[name].data
        unit = hdus[name].header["BUNIT"]
        name = name.replace("DATA", "FLUX")
        arrays[name] = xr.Variable(dims, data, attrs={"unit": str(unit)})
    return xr.Dataset(arrays, coords)


read_l1_data = to_dataset(_l1_data_reader)

# %% ../nbs/01_data.ipynb 79
def _class_table_reader(fn):
    """Read the CLASS_TABLE from a WEAVE L2 FITS file as a Dataset.

    All quantities are indexed by the `APS_ID` of the fibre.
    Chi-square values `CZZ_CHI2_*` for each template are further indexed by redshift `CZZ_*`.
    Coefficients `COEFF` and indexed by integers `I_COEFF`.
    """
    cols = _read_fits_columns(fn, "CLASS_TABLE")
    if not cols:
        return None
    coords = dict(APS_ID=cols.pop("APS_ID"))
    # convert CZZ columns to coordinates
    for c in list(cols):
        if c.startswith("CZZ") and "CHI2" not in c:
            czz_all = cols.pop(c)
            czz = czz_all[0]
            assert (czz == czz_all).all()
            coords[c] = czz
    for c in cols:
        dims = ["APS_ID"]
        if c.startswith("CZZ"):
            dims += [c.replace("_CHI2", "")]
        elif c == "COEFF":
            dims += ["I_COEFF"]
        cols[c] = xr.Variable(dims, cols[c], attrs={"unit": str(cols[c].unit)})
    return xr.Dataset(cols, coords)


read_class_table = to_dataset(_class_table_reader)

# %% ../nbs/01_data.ipynb 86
def _star_table_reader(fn):
    """Read the STAR_TABLE from a WEAVE L2 FITS file as a Dataset.

    All quantities are indexed by the `APS_ID` of the fibre.
    The covariance matrix `COVAR` is additionally indexed by `I_COVAR`, `J_COVAR`.
    The elements `ELEM` and `ELEM_ERR` are additionally indexed by `I_ELEM`.
    """
    cols = _read_fits_columns(fn, "STAR_TABLE")
    if not cols:
        return None
    coords = dict(APS_ID=cols.pop("APS_ID"))
    coords["I_COVAR"] = coords["J_COVAR"] = ["TEFF", "LOGG", "FEH", "ALPHA", "MICRO"]
    for c in cols:
        dims = ["APS_ID"]
        if c == "COVAR":
            dims += ["I_COVAR", "J_COVAR"]
        elif "ELEM" in c:
            dims += ["I_ELEM"]
        cols[c] = xr.Variable(dims, cols[c], attrs={"unit": str(cols[c].unit)})
    return xr.Dataset(cols, coords)


read_star_table = to_dataset(_star_table_reader)

# %% ../nbs/01_data.ipynb 93
def _not_line_col(c):
    """Identify columns that do not contain line measurements."""
    c = c.replace("ERR_", "")
    for n in ["EBMV0", "EBMV1", "FLUX", "AMPL", "Z", "SIGMA", "AON", "FWHM"]:
        if c.startswith(n + "_"):
            return False
    return True


def _process_line_quantities(cols, lines):
    """Process line quantities.

    Quantities with multiple elements are split into separate columns.

    The supplied `cols` dictionary is modified in-place.
    """
    line_quantities = []
    for c in list(cols):
        if c.endswith(lines[0]):
            qty = c.replace("_" + lines[0], "")
            ndim = cols[c].ndim
            if ndim == 1:
                line_quantities.append(qty)
            elif ndim == 2:
                nq = cols[c].shape[1]
                for i in range(nq):
                    line_quantities.append(f"{qty}{i}")
                for line in lines:
                    oldcol = cols.pop(f"{qty}_{line}")
                    for i in range(nq):
                        cols[f"{qty}{i}_{line}"] = oldcol[:, i]
    return line_quantities


def _galaxy_table_reader(fn):
    """Read the GALAXY_TABLE from a WEAVE L2 FITS file as a Dataset.

    All quantities are indexed by the `APS_ID` of the fibre.
    The line measurements are additionally indexed by the measurement quantity `QTY`
    and the line name `LINE`.
    The index measurements are additionally indexed by the index name `INDEX`.
    """
    # TODO: add units where missing
    cols = _read_fits_columns(fn, "GALAXY_TABLE")
    if not cols:
        return None
    coords = dict(APS_ID=cols.pop("APS_ID"))
    coords["LINE"] = [c.replace("FLUX_", "") for c in cols if c.startswith("FLUX")]
    coords["QTY"] = _process_line_quantities(cols, coords["LINE"])
    coords["INDEX"] = [c for i, c in enumerate(cols) if (i > 100 and _not_line_col(c))]
    line_cols = [
        [cols.pop(f"{qty}_{line}") for line in coords["LINE"]] for qty in coords["QTY"]
    ]
    index_cols = [cols.pop(idx) for idx in coords["INDEX"]]
    out_cols = {}
    for i, c in enumerate(cols):
        dims = ["APS_ID"]
        out_cols[c] = xr.Variable(dims, cols[c], attrs={"unit": str(cols[c].unit)})
    out_cols["LINES"] = xr.Variable(["QTY", "LINE", "APS_ID"], line_cols)
    out_cols["INDICES"] = xr.Variable(["INDEX", "APS_ID"], index_cols)
    return xr.Dataset(out_cols, coords)


read_galaxy_table = to_dataset(_galaxy_table_reader)

# %% ../nbs/01_data.ipynb 100
def _class_spec_reader(fn):
    """Read the CLASS_SPEC from a WEAVE L2 FITS file as a Dataset.

    All quantities are indexed by the `APS_ID` of the fibre.
    Spectral quantities are additionally indexed by wavelength `LAMBDA_{B,R}`.
    """
    cols = _read_fits_columns(fn, "CLASS_SPEC", limit_precision=True)
    if not cols:
        return None
    coords = dict(APS_ID=cols.pop("APS_ID"))
    for c in list(cols):
        if c.startswith("LAMBDA"):
            band = c.split("_")[-1]
            wl_all = cols.pop(c)
            wl = wl_all[0]
            assert (wl == wl_all).all()
            coords[f"LAMBDA_{band}"] = wl
    for c in cols:
        dims = ["APS_ID"]
        if c.endswith("_B"):
            dims += ["LAMBDA_B"]
        elif c.endswith("_R"):
            dims += ["LAMBDA_R"]
        cols[c] = xr.Variable(dims, cols[c], attrs={"unit": str(cols[c].unit)})
    return xr.Dataset(cols, coords)


read_class_spec = to_dataset(_class_spec_reader)

# %% ../nbs/01_data.ipynb 107
def _star_spec_reader(fn):
    """Read the STAR_SPEC from a WEAVE L2 FITS file as a Dataset.

    All quantities are indexed by the `APS_ID` of the fibre.
    Spectral quantities are additionally indexed by wavelength bin `LAMBIN_{R,B,C}`,
    which does *not* correspond to the same wavelength for each spectrum.
    """
    cols = _read_fits_columns(fn, "STAR_SPEC", limit_precision=True)
    if not cols:
        return None
    coords = dict(APS_ID=cols.pop("APS_ID"))
    for c in cols:
        dims = ["APS_ID"]
        if c.endswith("_B"):
            dims += ["LAMBIN_B"]
        elif c.endswith("_R"):
            dims += ["LAMBIN_R"]
        elif c.endswith("_C"):
            dims += ["LAMBIN_C"]
        cols[c] = xr.Variable(dims, cols[c], attrs={"unit": str(cols[c].unit)})
    return xr.Dataset(cols, coords)


read_star_spec = to_dataset(_star_spec_reader)

# %% ../nbs/01_data.ipynb 114
def _galaxy_spec_reader(fn):
    """Read the GALAXY_SPEC from a WEAVE L2 FITS file as a Dataset.

    All quantities are indexed by the `APS_ID` of the fibre.
    Spectral quantities are additionally indexed by log-wavelength bin `LOGLAMBIN`,
    which does *not* correspond to the same wavelength for each spectrum.
    """
    cols = _read_fits_columns(fn, "GALAXY_SPEC", limit_precision=True)
    if not cols:
        return None
    coords = dict(APS_ID=cols.pop("APS_ID"))
    for c in cols:
        dims = ["APS_ID"]
        if c.endswith("_PPXF") or c.endswith("_GAND"):
            dims += ["LOGLAMBIN"]
        cols[c] = xr.Variable(dims, cols[c], attrs={"unit": str(cols[c].unit)})
    return xr.Dataset(cols, coords)


read_galaxy_spec = to_dataset(_galaxy_spec_reader)
